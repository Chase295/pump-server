# Lokale Entwicklung - mit direkten Port-Mappings
# Für Coolify verwende docker-compose.yml (ohne Port-Mappings)

services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ml-prediction-service-backend
    restart: unless-stopped
    environment:
      - DB_DSN=postgresql://postgres:yc58TlS4Cz1j%26HHoy%25Oa@komodo.chase295.lo:5432/crypto_db
      - TRAINING_SERVICE_API_URL=${TRAINING_SERVICE_API_URL}
      - MODEL_STORAGE_PATH=/app/models
      - N8N_WEBHOOK_URL=${N8N_WEBHOOK_URL:-}
    volumes:
      - ./models:/app/models
      - ./logs:/app/logs
      - ./tmp:/tmp/models
      - ./config:/app/config
    ports:
      - "8001:8000"  # Direkter Zugriff auf Backend-API (Port 8001 für lokale Entwicklung)
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/docs"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - ml-prediction-network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.frontend
    container_name: ml-prediction-service-frontend
    restart: unless-stopped
    ports:
      - "3003:3000"  # Direkter Zugriff auf Frontend
    networks:
      - ml-prediction-network

networks:
  ml-prediction-network:
    driver: bridge
