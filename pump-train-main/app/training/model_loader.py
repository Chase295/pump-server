"""
Modell-Loader und Testing f√ºr ML Training Service
L√§dt gespeicherte Modelle und testet sie auf neuen Daten
"""
import joblib
import logging
from typing import Dict, Any
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    roc_auc_score, confusion_matrix, matthews_corrcoef
)
from app.training.feature_engineering import load_training_data, create_labels, create_time_based_labels, check_overlap
from datetime import timedelta, datetime as dt
from app.database.models import get_model

logger = logging.getLogger(__name__)

def load_model(model_path: str) -> Any:
    """
    L√§dt gespeichertes Modell aus .pkl Datei.
    
    Verwendet joblib zum Laden von trainierten Scikit-learn oder XGBoost Modellen.
    
    Args:
        model_path: Pfad zur .pkl Datei (z.B. "/app/models/model_random_forest_123.pkl")
    
    Returns:
        Geladenes Modell-Objekt (RandomForestClassifier, XGBClassifier, etc.)
        
    Raises:
        FileNotFoundError: Wenn die Datei nicht existiert
        Exception: Bei Fehlern beim Laden (z.B. korrupte Datei)
        
    Example:
        ```python
        model = load_model("/app/models/model_random_forest_123.pkl")
        predictions = model.predict(X_test)
        ```
    """
    logger.info(f"üìÇ Lade Modell: {model_path}")
    return joblib.load(model_path)

async def test_model(
    model_id: int,
    test_start: str,
    test_end: str,
    model_storage_path: str = "/app/models"
) -> Dict[str, Any]:
    """
    Testet ein Modell auf neuen Daten
    
    Args:
        model_id: ID des Modells aus ml_models
        test_start: Start-Zeitpunkt f√ºr Test-Daten (ISO-Format oder datetime)
        test_end: Ende-Zeitpunkt f√ºr Test-Daten (ISO-Format oder datetime)
        model_storage_path: Pfad zum Models-Verzeichnis (wird nicht verwendet, aber f√ºr Konsistenz)
    
    Returns:
        Dict mit allen Metriken, Confusion Matrix, Overlap-Info
    """
    logger.info(f"üß™ Starte Test f√ºr Modell {model_id}")
    
    # 1. Lade Modell-Info aus DB
    model = await get_model(model_id)
    if not model or model.get('is_deleted'):
        raise ValueError(f"Modell {model_id} nicht gefunden oder gel√∂scht!")
    
    logger.info(f"üìã Modell: {model['name']} ({model['model_type']})")
    
    # 2. Lade Modell-Datei
    model_obj = load_model(model['model_file_path'])
    
    # 3. Features und Phasen aus JSONB (asyncpg konvertiert automatisch)
    features = model['features']  # JSONB Array ‚Üí Python List (kann engineered features enthalten!)
    phases = model['phases'] if model['phases'] else None  # JSONB Array ‚Üí Python List
    
    logger.info(f"üìä Features: {features}, Phasen: {phases}")
    
    # 3.5. Pr√ºfe ob Feature-Engineering verwendet wurde
    params = model.get('params', {}) or {}
    logger.info(f"üîç Modell-params roh: {params}")
    if isinstance(params, str):
        from app.database.utils import from_jsonb
        params = from_jsonb(params) or {}
    logger.info(f"üîç Modell-params geparst: {params}")
    
    use_engineered_features = params.get('use_engineered_features', False)
    feature_engineering_windows = params.get('feature_engineering_windows', [5, 10, 15])
    logger.info(f"üîç use_engineered_features: {use_engineered_features}")
    
    if use_engineered_features:
        logger.info(f"üîß Modell wurde mit Feature-Engineering trainiert (Windows: {feature_engineering_windows})")
    
    # 3.6. Pr√ºfe ob zeitbasierte Vorhersage (target_operator ist NULL)
    is_time_based = model.get('target_operator') is None or model.get('target_value') is None
    
    # 4. Bestimme Basis-Features (ohne engineered features)
    # Engineered features haben Namen wie "price_change_5", "volume_ratio_10", etc.
    from app.training.feature_engineering import get_engineered_feature_names
    if use_engineered_features:
        engineered_feature_names = get_engineered_feature_names(feature_engineering_windows)
        # Basis-Features sind alle Features die NICHT engineered sind
        # ‚ö†Ô∏è WICHTIG: Konvertiere zu Set f√ºr schnelleren Lookup
        engineered_set = set(engineered_feature_names)
        base_features = [f for f in features if f not in engineered_set]
        
        # Debug: Pr√ºfe ob Filterung funktioniert
        still_engineered = [f for f in base_features if f in engineered_set]
        if still_engineered:
            logger.warning(f"‚ö†Ô∏è WARNUNG: {len(still_engineered)} engineered Features noch in base_features: {still_engineered[:5]}")
            # Entferne sie manuell
            base_features = [f for f in base_features if f not in engineered_set]
        
        logger.info(f"üìä Basis-Features: {len(base_features)}, Engineered Features: {len(engineered_feature_names)}")
        logger.debug(f"   Basis (erste 10): {base_features[:10]}")
        logger.debug(f"   Engineered (erste 10): {list(engineered_set)[:10]}")
    else:
        base_features = list(features)
        logger.info(f"üìä Basis-Features: {len(base_features)} (kein Feature-Engineering)")
    
    # 5. Lade Test-Daten
    # Verwende die gleichen Features wie beim Training f√ºr Daten-Laden
    # Engineered features werden sp√§ter hinzugef√ºgt
    features_for_loading = list(base_features)  # Nur Basis-Features f√ºr Daten-Laden

    # Stelle sicher, dass target_variable immer geladen wird (f√ºr Label-Erstellung)
    if model['target_variable'] not in features_for_loading:
        features_for_loading.append(model['target_variable'])
        logger.info(f"üéØ Target-Variable '{model['target_variable']}' zu Lade-Features hinzugef√ºgt")

    # üÜï F√ºr Tests: ATH-Daten immer deaktivieren (vereinfacht die Test-Logik)
    # ATH-Daten sind oft nicht verf√ºgbar und f√ºhren zu Fehlern
    include_ath = False  # Immer False f√ºr Tests

    logger.info(f"üìÇ Lade Test-Daten mit Basis-Features: {len(features_for_loading)}")
    logger.info(f"üèÜ include_ath: {include_ath} (immer False f√ºr Tests)")

    try:
        logger.info(f"üîç Lade Test-Daten von {test_start} bis {test_end}")
        logger.info(f"üìä Features f√ºr Laden: {features_for_loading}")
        logger.info(f"üé™ Phasen: {phases}")
        
        test_data = await load_training_data(
            train_start=test_start,
            train_end=test_end,
            features=features_for_loading,
            phases=phases,
            include_ath=include_ath  # üÜï ATH-Daten optional laden
        )

        logger.info(f"‚úÖ Test-Daten geladen: {len(test_data)} Zeilen, {len(test_data.columns)} Spalten")
        
        if len(test_data) == 0:
            logger.error("‚ùå KEINE Test-Daten gefunden!")
            # Zeige verf√ºgbare Zeitr√§ume
            from app.database.connection import get_pool
            pool = await get_pool()
            result = await pool.fetchrow("""
                SELECT
                    MIN(timestamp) as min_time,
                    MAX(timestamp) as max_time,
                    COUNT(*) as total_rows
                FROM coin_metrics
            """)
            if result:
                logger.info(f"üìä Datenbank-Info: {result['min_time']} bis {result['max_time']}, {result['total_rows']} Zeilen")
                logger.error(f"‚ùå Test-Zeitraum: {test_start} bis {test_end} hat keine Daten!")
                logger.error(f"üí° Verwende einen Zeitraum innerhalb: {result['min_time']} bis {result['max_time']}")
            raise ValueError(f"Keine Test-Daten gefunden! Verwende einen Zeitraum innerhalb der verf√ºgbaren Daten.")

    except Exception as e:
        logger.error(f"‚ùå Fehler beim Laden der Test-Daten: {e}")
        raise
    
    # 6. Feature-Engineering anwenden (genau wie beim Training!)
    if use_engineered_features:
        logger.info(f"üîß Generiere engineered features f√ºr Test-Daten (Windows: {feature_engineering_windows})...")
        from app.training.feature_engineering import create_pump_detection_features
        
        # NEU: Pr√ºfe ob Modell Flag-Features verwendet
        use_flag_features = model.get('use_flag_features', True)  # Standard: True f√ºr neue Modelle
        
        # Erstelle DIE GLEICHEN engineered features wie beim Training
        test_data = create_pump_detection_features(
            test_data, 
            window_sizes=feature_engineering_windows,
            include_flags=use_flag_features  # NEU
        )

        logger.info(f"üìä Nach Feature-Engineering: {len(test_data.columns)} Spalten verf√ºgbar")

        # Pr√ºfe ob alle Features vorhanden sind, die das Modell erwartet
        model_features = set(features)  # Features mit denen das Modell trainiert wurde
        available_features = set(test_data.columns)
        missing_features = model_features - available_features

        # F√ºge fehlende Features mit Default-Wert 0 hinzu
        if missing_features:
            logger.warning(f"‚ö†Ô∏è {len(missing_features)} Features fehlen in Test-Daten: {list(missing_features)}")
            logger.info("üîß F√ºge fehlende Features mit Default-Wert 0 hinzu, um Shape-Mismatch zu vermeiden")

            for missing_feature in missing_features:
                test_data[missing_feature] = 0.0
                logger.debug(f"‚ûï Feature '{missing_feature}' mit Default-Wert 0 hinzugef√ºgt")

        # Aktualisiere features, um alle verf√ºgbaren Features zu enthalten
        # WICHTIG: Das Modell wurde mit diesen Features trainiert, also m√ºssen wir genau diese verwenden
        available_model_features = [f for f in model_features if f in test_data.columns]
        features = available_model_features  # Aktualisiere features f√ºr die weitere Verarbeitung

        logger.info(f"‚úÖ Features aktualisiert: {len(features)} von {len(model_features)} verf√ºgbar")

        # Stelle sicher, dass genau die erwarteten Features vorhanden sind
        # ‚ö†Ô∏è WICHTIG: target_variable MUSS immer erhalten bleiben f√ºr Label-Erstellung!
        expected_features = set(model_features)
        target_var = model['target_variable']
        expected_features.add(target_var)  # üîß FIX: target_variable immer hinzuf√ºgen
        
        available_features = set(test_data.columns)
        extra_features = available_features - expected_features

        if extra_features:
            logger.info(f"üßπ Entferne {len(extra_features)} unerwartete Features (behalte target_var '{target_var}')")
            test_data = test_data[list(expected_features)]
            logger.info(f"‚úÖ DataFrame hat jetzt genau {len(expected_features)} erwartete Features")

        # Finale √úberpr√ºfung
        final_features = set(test_data.columns)
        if final_features == expected_features:
            logger.info(f"üéØ Perfekt: DataFrame hat genau {len(expected_features)} erwartete Features")
        else:
            still_missing = expected_features - final_features
            if still_missing:
                logger.error(f"‚ùå Immer noch {len(still_missing)} Features fehlen: {list(still_missing)}")
                # F√ºge die restlichen fehlenden Features hinzu
                for missing in still_missing:
                    test_data[missing] = 0.0
                    logger.info(f"‚ûï Nachgetragen: '{missing}' = 0.0")
            still_extra = final_features - expected_features
            if still_extra:
                logger.warning(f"‚ö†Ô∏è {len(still_extra)} Extra-Features werden entfernt")
                # üîß FIX: Stelle sicher, dass target_var immer enthalten ist
                columns_to_keep = list(expected_features)
                if target_var not in columns_to_keep:
                    columns_to_keep.append(target_var)
                test_data = test_data[columns_to_keep]

        # Finale Pr√ºfung: target_variable MUSS vorhanden sein
        if model['target_variable'] not in test_data.columns:
            logger.error(f"‚ùå target_variable '{model['target_variable']}' fehlt in Test-Daten!")
            logger.info(f"üìã Verf√ºgbare Spalten: {list(test_data.columns)[:10]}...")
            raise ValueError(f"Target variable {model['target_variable']} nicht in Test-Daten gefunden")
    else:
        # Auch ohne Feature-Engineering: Pr√ºfe ob alle Features vorhanden sind
        missing_features = [f for f in features if f not in test_data.columns]
        if missing_features:
            logger.warning(f"‚ö†Ô∏è {len(missing_features)} Features fehlen in Test-Daten: {missing_features}")
            logger.info("üîß F√ºge fehlende Features mit Default-Wert 0 hinzu, um Shape-Mismatch zu vermeiden")

            # F√ºge fehlende Features mit Default-Wert 0 hinzu
            for missing_feature in missing_features:
                test_data[missing_feature] = 0.0
                logger.debug(f"‚ûï Feature '{missing_feature}' mit Default-Wert 0 hinzugef√ºgt")

            logger.info(f"‚úÖ Shape-Mismatch behoben: Alle {len(features)} Modell-Features verf√ºgbar")
    
    # 7. Erstelle Labels (gleiche Logik wie beim Training)
    if is_time_based:
        # Zeitbasierte Vorhersage: Hole Parameter aus params
        params = model.get('params', {}) or {}
        if isinstance(params, str):
            import json
            try:
                from app.database.utils import from_jsonb
                params = from_jsonb(params) or {}
            except:
                params = {}
        
        time_based_config = params.get('_time_based', {})
        
        future_minutes = time_based_config.get('future_minutes')
        min_percent_change = time_based_config.get('min_percent_change')
        direction = time_based_config.get('direction', 'up')
        
        # Fallback: Falls Parameter fehlen, versuche aus Job-Daten zu holen (f√ºr √§ltere Modelle)
        if not future_minutes or min_percent_change is None:
            logger.warning(f"‚ö†Ô∏è Zeitbasierte Parameter fehlen in Modell {model_id}, versuche Fallback...")
            # Versuche aus ml_jobs zu holen (falls Modell k√ºrzlich erstellt wurde)
            from app.database.models import get_job
            # Suche nach TRAIN-Job f√ºr dieses Modell
            # Da wir keine direkte Verkn√ºpfung haben, verwenden wir Default-Werte
            if future_minutes is None:
                future_minutes = 10  # Default
                logger.warning(f"‚ö†Ô∏è Verwende Default future_minutes={future_minutes}")
            if min_percent_change is None:
                min_percent_change = 5.0  # Default
                logger.warning(f"‚ö†Ô∏è Verwende Default min_percent_change={min_percent_change}")
            if not direction:
                direction = 'up'
        
        if not future_minutes or min_percent_change is None:
            raise ValueError(f"Zeitbasierte Vorhersage-Parameter fehlen in Modell {model_id}! future_minutes={future_minutes}, min_percent_change={min_percent_change}")
        
        # Lade Phase-Intervalle
        from app.database.models import get_phase_intervals
        phase_intervals = await get_phase_intervals()
        
        from app.training.feature_engineering import create_time_based_labels
        labels = create_time_based_labels(
            test_data,
            model['target_variable'],
            future_minutes,
            min_percent_change,
            direction,
            phase_intervals
        )
        logger.info(f"‚è∞ Zeitbasierte Labels erstellt: {labels.sum()} positive, {len(labels) - labels.sum()} negative")
    else:
        # Normale Vorhersage
        if model.get('target_operator') is None or model.get('target_value') is None:
            raise ValueError(f"Modell {model_id} hat keine target_operator/target_value f√ºr normale Vorhersage!")
        
        labels = create_labels(
            test_data, 
            model['target_variable'],
            model['target_operator'],
            float(model['target_value'])
        )
        logger.info(f"‚úÖ Normale Labels erstellt: {labels.sum()} positive, {len(labels) - labels.sum()} negative")
    
    # 8. Mache Vorhersagen (verwende alle Features, nicht target_var)
    # target_var wurde nur f√ºr Labels ben√∂tigt, nicht f√ºr Features
    # ‚ö†Ô∏è WICHTIG: features enth√§lt jetzt auch engineered features (wenn Feature-Engineering aktiviert)
    # ‚ö†Ô∏è Nach dem Auff√ºllen oben sollten alle Features verf√ºgbar sein
    available_features = [f for f in features if f in test_data.columns]
    if len(available_features) != len(features):
        missing = [f for f in features if f not in test_data.columns]
        logger.error(f"‚ùå KRITISCH: Trotz Auff√ºllen fehlen {len(missing)} Features: {missing}")
        raise ValueError(f"Features fehlen in Test-Daten nach Auff√ºllen: {missing}")
    
    logger.info(f"üéØ Verwende {len(available_features)} Features f√ºr Vorhersage")
    X_test = test_data[available_features].values
    y_test = labels.values
    y_pred = model_obj.predict(X_test)
    y_pred_proba = model_obj.predict_proba(X_test)[:, 1] if hasattr(model_obj, 'predict_proba') else None
    
    logger.info(f"üîÆ Vorhersagen gemacht: {len(y_pred)} Samples mit {len(features)} Features")
    
    # 9. Berechne Basis-Metriken
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None
    
    logger.info(f"üìà Basis-Metriken: Accuracy={accuracy:.4f}, F1={f1:.4f}, Precision={precision:.4f}, Recall={recall:.4f}")
    
    # 10. Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    if cm.size == 4:  # 2x2 Matrix
        tn, fp, fn, tp = cm.ravel()
    else:
        # Fallback falls Matrix anders strukturiert
        tn, fp, fn, tp = 0, 0, 0, 0
    
    # 11. Zus√§tzliche Metriken (Phase 9)
    # MCC (Matthews Correlation Coefficient)
    mcc = matthews_corrcoef(y_test, y_pred)
    
    # FPR (False Positive Rate)
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
    
    # FNR (False Negative Rate)
    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0
    
    # Profit-Simulation (vereinfacht)
    # Annahme: 1% Gewinn pro richtig erkanntem Pump, 0.5% Verlust pro False Positive
    profit_per_tp = 0.01  # 1%
    loss_per_fp = -0.005  # -0.5%
    simulated_profit = (tp * profit_per_tp) + (fp * loss_per_fp)
    simulated_profit_pct = simulated_profit / len(y_test) * 100 if len(y_test) > 0 else 0.0
    
    # Confusion Matrix als Dict
    confusion_matrix_dict = {
        "tp": int(tp),
        "tn": int(tn),
        "fp": int(fp),
        "fn": int(fn)
    }
    
    logger.info(f"üí∞ Simulierter Profit: {simulated_profit_pct:.2f}% (bei {tp} TP, {fp} FP)")
    roc_auc_str = f"{roc_auc:.4f}" if roc_auc is not None else "N/A"
    logger.info(f"üìä Zus√§tzliche Metriken: ROC-AUC={roc_auc_str}, MCC={mcc:.4f}, FPR={fpr:.4f}, FNR={fnr:.4f}")
    
    # 12. Test-Zeitraum Validierung (Phase 2)
    if isinstance(test_start, str):
        test_start_dt = dt.fromisoformat(test_start.replace('Z', '+00:00'))
    else:
        test_start_dt = test_start
    if isinstance(test_end, str):
        test_end_dt = dt.fromisoformat(test_end.replace('Z', '+00:00'))
    else:
        test_end_dt = test_end
    
    test_duration = test_end_dt - test_start_dt
    min_test_duration = timedelta(days=1)  # Mindest 1 Tag
    test_duration_days = test_duration.total_seconds() / 86400.0  # In Tagen
    
    if test_duration < min_test_duration:
        logger.warning(f"‚ö†Ô∏è Test-Zeitraum zu kurz: {test_duration_days:.2f} Tage (empfohlen: mindestens 1 Tag)")
    
    # 13. Overlap-Check
    overlap_info = check_overlap(
        train_start=model['train_start'],
        train_end=model['train_end'],
        test_start=test_start,
        test_end=test_end
    )
    
    if overlap_info['has_overlap']:
        logger.warning(f"{overlap_info['overlap_note']}")
    
    # 14. Train vs. Test Vergleich (Phase 2)
    train_accuracy = model.get('training_accuracy')
    train_f1 = model.get('training_f1')
    train_precision = model.get('training_precision')
    train_recall = model.get('training_recall')
    
    accuracy_degradation = None
    f1_degradation = None
    is_overfitted = None
    
    if train_accuracy is not None:
        accuracy_degradation = float(train_accuracy) - float(accuracy)
        # Overfitting-Indikator: > 10% Unterschied
        is_overfitted = accuracy_degradation > 0.1
        
        if is_overfitted:
            logger.warning(f"‚ö†Ô∏è OVERFITTING erkannt! Train-Test Accuracy Gap: {accuracy_degradation:.2%}")
            logger.warning(f"   ‚Üí Modell generalisiert schlecht auf neue Daten")
        else:
            logger.info(f"‚úÖ Train-Test Accuracy Gap: {accuracy_degradation:.2%} (akzeptabel)")
    
    if train_f1 is not None:
        f1_degradation = float(train_f1) - float(f1)
        logger.info(f"üìä Train-Test F1 Gap: {f1_degradation:.2%}")
    
    # 15. Return Ergebnisse
    result = {
        "accuracy": float(accuracy),
        "f1_score": float(f1),
        "precision_score": float(precision),
        "recall": float(recall),
        "roc_auc": float(roc_auc) if roc_auc is not None else None,
        # Zus√§tzliche Metriken (Phase 9)
        "mcc": float(mcc),
        "fpr": float(fpr),
        "fnr": float(fnr),
        "simulated_profit_pct": float(simulated_profit_pct),
        "confusion_matrix": confusion_matrix_dict,
        # Legacy: Confusion Matrix als einzelne Felder (f√ºr R√ºckw√§rtskompatibilit√§t)
        "tp": int(tp),
        "tn": int(tn),
        "fp": int(fp),
        "fn": int(fn),
        "num_samples": len(test_data),
        "num_positive": int(labels.sum()),
        "num_negative": int(len(labels) - labels.sum()),
        "has_overlap": overlap_info['has_overlap'],
        "overlap_note": overlap_info['overlap_note'],
        # Train vs. Test Vergleich (Phase 2)
        "train_accuracy": float(train_accuracy) if train_accuracy is not None else None,
        "train_f1": float(train_f1) if train_f1 is not None else None,
        "train_precision": float(train_precision) if train_precision is not None else None,
        "train_recall": float(train_recall) if train_recall is not None else None,
        "accuracy_degradation": float(accuracy_degradation) if accuracy_degradation is not None else None,
        "f1_degradation": float(f1_degradation) if f1_degradation is not None else None,
        "is_overfitted": bool(is_overfitted) if is_overfitted is not None else None,
        # Test-Zeitraum Info (Phase 2)
        "test_duration_days": float(test_duration_days)
    }
    
    logger.info(f"‚úÖ Test abgeschlossen f√ºr Modell {model_id}")
    
    return result


    confusion_matrix_dict = {
        "tp": int(tp),
        "tn": int(tn),
        "fp": int(fp),
        "fn": int(fn)
    }
    
    logger.info(f"üí∞ Simulierter Profit: {simulated_profit_pct:.2f}% (bei {tp} TP, {fp} FP)")
    roc_auc_str = f"{roc_auc:.4f}" if roc_auc is not None else "N/A"
    logger.info(f"üìä Zus√§tzliche Metriken: ROC-AUC={roc_auc_str}, MCC={mcc:.4f}, FPR={fpr:.4f}, FNR={fnr:.4f}")
    
    # 12. Test-Zeitraum Validierung (Phase 2)
    if isinstance(test_start, str):
        test_start_dt = dt.fromisoformat(test_start.replace('Z', '+00:00'))
    else:
        test_start_dt = test_start
    if isinstance(test_end, str):
        test_end_dt = dt.fromisoformat(test_end.replace('Z', '+00:00'))
    else:
        test_end_dt = test_end
    
    test_duration = test_end_dt - test_start_dt
    min_test_duration = timedelta(days=1)  # Mindest 1 Tag
    test_duration_days = test_duration.total_seconds() / 86400.0  # In Tagen
    
    if test_duration < min_test_duration:
        logger.warning(f"‚ö†Ô∏è Test-Zeitraum zu kurz: {test_duration_days:.2f} Tage (empfohlen: mindestens 1 Tag)")
    
    # 13. Overlap-Check
    overlap_info = check_overlap(
        train_start=model['train_start'],
        train_end=model['train_end'],
        test_start=test_start,
        test_end=test_end
    )
    
    if overlap_info['has_overlap']:
        logger.warning(f"{overlap_info['overlap_note']}")
    
    # 14. Train vs. Test Vergleich (Phase 2)
    train_accuracy = model.get('training_accuracy')
    train_f1 = model.get('training_f1')
    train_precision = model.get('training_precision')
    train_recall = model.get('training_recall')
    
    accuracy_degradation = None
    f1_degradation = None
    is_overfitted = None
    
    if train_accuracy is not None:
        accuracy_degradation = float(train_accuracy) - float(accuracy)
        # Overfitting-Indikator: > 10% Unterschied
        is_overfitted = accuracy_degradation > 0.1
        
        if is_overfitted:
            logger.warning(f"‚ö†Ô∏è OVERFITTING erkannt! Train-Test Accuracy Gap: {accuracy_degradation:.2%}")
            logger.warning(f"   ‚Üí Modell generalisiert schlecht auf neue Daten")
        else:
            logger.info(f"‚úÖ Train-Test Accuracy Gap: {accuracy_degradation:.2%} (akzeptabel)")
    
    if train_f1 is not None:
        f1_degradation = float(train_f1) - float(f1)
        logger.info(f"üìä Train-Test F1 Gap: {f1_degradation:.2%}")
    
    # 15. Return Ergebnisse
    result = {
        "accuracy": float(accuracy),
        "f1_score": float(f1),
        "precision_score": float(precision),
        "recall": float(recall),
        "roc_auc": float(roc_auc) if roc_auc is not None else None,
        # Zus√§tzliche Metriken (Phase 9)
        "mcc": float(mcc),
        "fpr": float(fpr),
        "fnr": float(fnr),
        "simulated_profit_pct": float(simulated_profit_pct),
        "confusion_matrix": confusion_matrix_dict,
        # Legacy: Confusion Matrix als einzelne Felder (f√ºr R√ºckw√§rtskompatibilit√§t)
        "tp": int(tp),
        "tn": int(tn),
        "fp": int(fp),
        "fn": int(fn),
        "num_samples": len(test_data),
        "num_positive": int(labels.sum()),
        "num_negative": int(len(labels) - labels.sum()),
        "has_overlap": overlap_info['has_overlap'],
        "overlap_note": overlap_info['overlap_note'],
        # Train vs. Test Vergleich (Phase 2)
        "train_accuracy": float(train_accuracy) if train_accuracy is not None else None,
        "train_f1": float(train_f1) if train_f1 is not None else None,
        "train_precision": float(train_precision) if train_precision is not None else None,
        "train_recall": float(train_recall) if train_recall is not None else None,
        "accuracy_degradation": float(accuracy_degradation) if accuracy_degradation is not None else None,
        "f1_degradation": float(f1_degradation) if f1_degradation is not None else None,
        "is_overfitted": bool(is_overfitted) if is_overfitted is not None else None,
        # Test-Zeitraum Info (Phase 2)
        "test_duration_days": float(test_duration_days)
    }
    
    logger.info(f"‚úÖ Test abgeschlossen f√ºr Modell {model_id}")
    
    return result


    confusion_matrix_dict = {
        "tp": int(tp),
        "tn": int(tn),
        "fp": int(fp),
        "fn": int(fn)
    }
    
    logger.info(f"üí∞ Simulierter Profit: {simulated_profit_pct:.2f}% (bei {tp} TP, {fp} FP)")
    roc_auc_str = f"{roc_auc:.4f}" if roc_auc is not None else "N/A"
    logger.info(f"üìä Zus√§tzliche Metriken: ROC-AUC={roc_auc_str}, MCC={mcc:.4f}, FPR={fpr:.4f}, FNR={fnr:.4f}")
    
    # 12. Test-Zeitraum Validierung (Phase 2)
    if isinstance(test_start, str):
        test_start_dt = dt.fromisoformat(test_start.replace('Z', '+00:00'))
    else:
        test_start_dt = test_start
    if isinstance(test_end, str):
        test_end_dt = dt.fromisoformat(test_end.replace('Z', '+00:00'))
    else:
        test_end_dt = test_end
    
    test_duration = test_end_dt - test_start_dt
    min_test_duration = timedelta(days=1)  # Mindest 1 Tag
    test_duration_days = test_duration.total_seconds() / 86400.0  # In Tagen
    
    if test_duration < min_test_duration:
        logger.warning(f"‚ö†Ô∏è Test-Zeitraum zu kurz: {test_duration_days:.2f} Tage (empfohlen: mindestens 1 Tag)")
    
    # 13. Overlap-Check
    overlap_info = check_overlap(
        train_start=model['train_start'],
        train_end=model['train_end'],
        test_start=test_start,
        test_end=test_end
    )
    
    if overlap_info['has_overlap']:
        logger.warning(f"{overlap_info['overlap_note']}")
    
    # 14. Train vs. Test Vergleich (Phase 2)
    train_accuracy = model.get('training_accuracy')
    train_f1 = model.get('training_f1')
    train_precision = model.get('training_precision')
    train_recall = model.get('training_recall')
    
    accuracy_degradation = None
    f1_degradation = None
    is_overfitted = None
    
    if train_accuracy is not None:
        accuracy_degradation = float(train_accuracy) - float(accuracy)
        # Overfitting-Indikator: > 10% Unterschied
        is_overfitted = accuracy_degradation > 0.1
        
        if is_overfitted:
            logger.warning(f"‚ö†Ô∏è OVERFITTING erkannt! Train-Test Accuracy Gap: {accuracy_degradation:.2%}")
            logger.warning(f"   ‚Üí Modell generalisiert schlecht auf neue Daten")
        else:
            logger.info(f"‚úÖ Train-Test Accuracy Gap: {accuracy_degradation:.2%} (akzeptabel)")
    
    if train_f1 is not None:
        f1_degradation = float(train_f1) - float(f1)
        logger.info(f"üìä Train-Test F1 Gap: {f1_degradation:.2%}")
    
    # 15. Return Ergebnisse
    result = {
        "accuracy": float(accuracy),
        "f1_score": float(f1),
        "precision_score": float(precision),
        "recall": float(recall),
        "roc_auc": float(roc_auc) if roc_auc is not None else None,
        # Zus√§tzliche Metriken (Phase 9)
        "mcc": float(mcc),
        "fpr": float(fpr),
        "fnr": float(fnr),
        "simulated_profit_pct": float(simulated_profit_pct),
        "confusion_matrix": confusion_matrix_dict,
        # Legacy: Confusion Matrix als einzelne Felder (f√ºr R√ºckw√§rtskompatibilit√§t)
        "tp": int(tp),
        "tn": int(tn),
        "fp": int(fp),
        "fn": int(fn),
        "num_samples": len(test_data),
        "num_positive": int(labels.sum()),
        "num_negative": int(len(labels) - labels.sum()),
        "has_overlap": overlap_info['has_overlap'],
        "overlap_note": overlap_info['overlap_note'],
        # Train vs. Test Vergleich (Phase 2)
        "train_accuracy": float(train_accuracy) if train_accuracy is not None else None,
        "train_f1": float(train_f1) if train_f1 is not None else None,
        "train_precision": float(train_precision) if train_precision is not None else None,
        "train_recall": float(train_recall) if train_recall is not None else None,
        "accuracy_degradation": float(accuracy_degradation) if accuracy_degradation is not None else None,
        "f1_degradation": float(f1_degradation) if f1_degradation is not None else None,
        "is_overfitted": bool(is_overfitted) if is_overfitted is not None else None,
        # Test-Zeitraum Info (Phase 2)
        "test_duration_days": float(test_duration_days)
    }
    
    logger.info(f"‚úÖ Test abgeschlossen f√ºr Modell {model_id}")
    
    return result


    confusion_matrix_dict = {
        "tp": int(tp),
        "tn": int(tn),
        "fp": int(fp),
        "fn": int(fn)
    }
    
    logger.info(f"üí∞ Simulierter Profit: {simulated_profit_pct:.2f}% (bei {tp} TP, {fp} FP)")
    roc_auc_str = f"{roc_auc:.4f}" if roc_auc is not None else "N/A"
    logger.info(f"üìä Zus√§tzliche Metriken: ROC-AUC={roc_auc_str}, MCC={mcc:.4f}, FPR={fpr:.4f}, FNR={fnr:.4f}")
    
    # 12. Test-Zeitraum Validierung (Phase 2)
    if isinstance(test_start, str):
        test_start_dt = dt.fromisoformat(test_start.replace('Z', '+00:00'))
    else:
        test_start_dt = test_start
    if isinstance(test_end, str):
        test_end_dt = dt.fromisoformat(test_end.replace('Z', '+00:00'))
    else:
        test_end_dt = test_end
    
    test_duration = test_end_dt - test_start_dt
    min_test_duration = timedelta(days=1)  # Mindest 1 Tag
    test_duration_days = test_duration.total_seconds() / 86400.0  # In Tagen
    
    if test_duration < min_test_duration:
        logger.warning(f"‚ö†Ô∏è Test-Zeitraum zu kurz: {test_duration_days:.2f} Tage (empfohlen: mindestens 1 Tag)")
    
    # 13. Overlap-Check
    overlap_info = check_overlap(
        train_start=model['train_start'],
        train_end=model['train_end'],
        test_start=test_start,
        test_end=test_end
    )
    
    if overlap_info['has_overlap']:
        logger.warning(f"{overlap_info['overlap_note']}")
    
    # 14. Train vs. Test Vergleich (Phase 2)
    train_accuracy = model.get('training_accuracy')
    train_f1 = model.get('training_f1')
    train_precision = model.get('training_precision')
    train_recall = model.get('training_recall')
    
    accuracy_degradation = None
    f1_degradation = None
    is_overfitted = None
    
    if train_accuracy is not None:
        accuracy_degradation = float(train_accuracy) - float(accuracy)
        # Overfitting-Indikator: > 10% Unterschied
        is_overfitted = accuracy_degradation > 0.1
        
        if is_overfitted:
            logger.warning(f"‚ö†Ô∏è OVERFITTING erkannt! Train-Test Accuracy Gap: {accuracy_degradation:.2%}")
            logger.warning(f"   ‚Üí Modell generalisiert schlecht auf neue Daten")
        else:
            logger.info(f"‚úÖ Train-Test Accuracy Gap: {accuracy_degradation:.2%} (akzeptabel)")
    
    if train_f1 is not None:
        f1_degradation = float(train_f1) - float(f1)
        logger.info(f"üìä Train-Test F1 Gap: {f1_degradation:.2%}")
    
    # 15. Return Ergebnisse
    result = {
        "accuracy": float(accuracy),
        "f1_score": float(f1),
        "precision_score": float(precision),
        "recall": float(recall),
        "roc_auc": float(roc_auc) if roc_auc is not None else None,
        # Zus√§tzliche Metriken (Phase 9)
        "mcc": float(mcc),
        "fpr": float(fpr),
        "fnr": float(fnr),
        "simulated_profit_pct": float(simulated_profit_pct),
        "confusion_matrix": confusion_matrix_dict,
        # Legacy: Confusion Matrix als einzelne Felder (f√ºr R√ºckw√§rtskompatibilit√§t)
        "tp": int(tp),
        "tn": int(tn),
        "fp": int(fp),
        "fn": int(fn),
        "num_samples": len(test_data),
        "num_positive": int(labels.sum()),
        "num_negative": int(len(labels) - labels.sum()),
        "has_overlap": overlap_info['has_overlap'],
        "overlap_note": overlap_info['overlap_note'],
        # Train vs. Test Vergleich (Phase 2)
        "train_accuracy": float(train_accuracy) if train_accuracy is not None else None,
        "train_f1": float(train_f1) if train_f1 is not None else None,
        "train_precision": float(train_precision) if train_precision is not None else None,
        "train_recall": float(train_recall) if train_recall is not None else None,
        "accuracy_degradation": float(accuracy_degradation) if accuracy_degradation is not None else None,
        "f1_degradation": float(f1_degradation) if f1_degradation is not None else None,
        "is_overfitted": bool(is_overfitted) if is_overfitted is not None else None,
        # Test-Zeitraum Info (Phase 2)
        "test_duration_days": float(test_duration_days)
    }
    
    logger.info(f"‚úÖ Test abgeschlossen f√ºr Modell {model_id}")
    
    return result

