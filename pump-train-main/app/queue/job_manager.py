"""
Job Manager f√ºr ML Training Service
Verarbeitet Jobs aus der ml_jobs Tabelle (TRAIN, TEST, COMPARE)
"""
import asyncio
import logging
from typing import Dict, Any, Optional
from datetime import datetime, timezone
from app.database.models import (
    get_next_pending_job, update_job_status, get_job,
    create_model, create_test_result, get_or_create_test_result, create_comparison, create_comparison_v2
)
from app.training.engine import train_model
from app.training.model_loader import test_model
from app.utils.config import JOB_POLL_INTERVAL, MAX_CONCURRENT_JOBS, MODEL_STORAGE_PATH
from app.utils.metrics import (
    ml_jobs_total, increment_job_counter, update_active_jobs,
    update_job_metrics, update_job_feature_metrics
)

logger = logging.getLogger(__name__)

async def process_job(job_id: int) -> None:
    """
    Verarbeitet einen einzelnen Job
    
    ‚ö†Ô∏è WICHTIG: CPU-bound Training l√§uft in run_in_executor!
    
    Args:
        job_id: ID des Jobs aus ml_jobs
    """
    job = await get_job(job_id)
    if not job:
        logger.error(f"‚ùå Job {job_id} nicht gefunden!")
        return
    
    job_type = job['job_type']
    logger.info(f"üöÄ Starte Job {job_id}: {job_type}")
    
    # Metriken aktualisieren
    increment_job_counter(job_type=job_type, status="running")
    update_active_jobs(1)
    
    try:
        # ‚ö†Ô∏è WICHTIG: Status ist bereits auf RUNNING gesetzt (in get_next_pending_job)
        # Nur Progress aktualisieren, falls n√∂tig
        # await update_job_status(job_id, status="RUNNING", progress=0.0)  # ENTFERNT - wird bereits in get_next_pending_job gemacht
        
        if job_type == "TRAIN":
            await process_train_job(job)
        elif job_type == "TEST":
            await process_test_job(job)
        elif job_type == "COMPARE":
            await process_compare_job(job)
        else:
            raise ValueError(f"Unbekannter Job-Typ: {job_type}")
        
        # Job erfolgreich abgeschlossen
        await update_job_status(job_id, status="COMPLETED", progress=1.0)
        increment_job_counter(job_type=job_type, status="completed")
        logger.info(f"‚úÖ Job {job_id} erfolgreich abgeschlossen")
        
    except Exception as e:
        # Job fehlgeschlagen
        error_msg = str(e)
        logger.error(f"‚ùå Job {job_id} fehlgeschlagen: {error_msg}", exc_info=True)
        
        # Update Metriken: Job fehlgeschlagen
        model_type = job.get('train_model_type') or job.get('test_model_id') or 'unknown'
        update_job_metrics(
            job_id=job_id,
            job_type=job_type,
            model_type=model_type,
            status='FAILED',
            progress=0.0,
            duration_seconds=None
        )
        
        await update_job_status(
            job_id,
            status="FAILED",
            progress=0.0,
            error_msg=error_msg
        )
        increment_job_counter(job_type=job_type, status="failed")
    finally:
        update_active_jobs(-1)

async def process_train_job(job: Dict[str, Any]) -> None:
    """
    Verarbeitet TRAIN Job: Erstellt neues Modell
    
    ‚ö†Ô∏è KRITISCH: Training l√§uft in run_in_executor (CPU-bound)!
    """
    job_id = job['id']
    logger.info(f"üéØ Verarbeite TRAIN Job {job_id}")
    logger.info(f"üìã Job-Details: Typ={job.get('train_model_type')}, Features={len(job.get('train_features', []))}, Phasen={job.get('train_phases')}")
    
    # 1. ‚ö†Ô∏è KRITISCH: Hole Modell-Name aus progress_msg BEVOR es √ºberschrieben wird!
    # Der Name wurde beim Job-Erstellen in progress_msg gespeichert
    model_name = job.get('progress_msg') or f"Model_{job_id}"
    logger.info(f"üìù Modell-Name: {model_name}")
    
    # ‚ö†Ô∏è WICHTIG: Speichere den urspr√ºnglichen Namen, falls progress_msg sp√§ter √ºberschrieben wird
    original_model_name = model_name
    
    # Update Metriken: Job gestartet
    model_type = job.get('train_model_type', 'unknown')
    features = job.get('train_features', [])
    phases = job.get('train_phases', [])
    update_job_metrics(
        job_id=job_id,
        job_type='TRAIN',
        model_type=model_type,
        status='RUNNING',
        progress=0.1,
        duration_seconds=0
    )
    update_job_feature_metrics(
        job_id=job_id,
        job_type='TRAIN',
        features_count=len(features) if isinstance(features, list) else 0,
        phases_count=len(phases) if isinstance(phases, list) else 0
    )
    
    # 2. Hole Job-Parameter (features und phases bereits oben geholt)
    target_var = job['train_target_var']  # Kann None sein wenn zeitbasierte Vorhersage aktiviert
    target_operator = job['train_operator']  # Kann None sein wenn zeitbasierte Vorhersage aktiviert
    target_value = float(job['train_value']) if job['train_value'] is not None else None  # Kann None sein
    train_start = job['train_start']
    train_end = job['train_end']
    params = job['train_params']  # JSONB ‚Üí Python Dict oder None oder String
    
    # NEU: Konvertiere params zu Dict falls es ein String ist
    if params is not None and isinstance(params, str):
        from app.database.utils import from_jsonb
        params = from_jsonb(params) or {}
    
    # NEU: Hole use_flag_features aus params (hat Priorit√§t) oder Job-Spalte (Fallback)
    # ‚ö†Ô∏è WICHTIG: params hat Priorit√§t, da es explizit vom Benutzer gesetzt wurde
    if params and isinstance(params, dict):
        use_flag_features = params.get('use_flag_features')
    
    # Fallback: Hole aus Job-Spalte
    if use_flag_features is None:
        use_flag_features = job.get('use_flag_features')
    
    # Fallback: Standard-Wert
    if use_flag_features is None:
        use_flag_features = True  # Standard: aktiviert
    
    # Stelle sicher, dass use_flag_features in params ist (f√ºr Modell-Speicherung)
    if params is None:
        params = {}
    params['use_flag_features'] = use_flag_features
    
    # NEU: Extrahiere zeitbasierte Parameter aus train_params
    use_time_based = False
    future_minutes = None
    min_percent_change = None
    direction = "up"
    
    if params and isinstance(params, dict) and "_time_based" in params:
        time_based_config = params.get("_time_based", {})
        use_time_based = time_based_config.get("enabled", False)
        future_minutes = time_based_config.get("future_minutes")
        min_percent_change = time_based_config.get("min_percent_change")
        direction = time_based_config.get("direction", "up")
        
        # ‚ö†Ô∏è WICHTIG: Behalte _time_based in params, damit es beim Testen verf√ºgbar ist!
        # params wird so an create_model √ºbergeben und in der DB gespeichert
        
        logger.info(f"‚è∞ Zeitbasierte Vorhersage aktiviert: {future_minutes} Min, {min_percent_change}%, {direction}")
    
    logger.info(f"üìã Training-Parameter: {model_type}, Features: {len(features) if features else 0}")
    
    # 3. Update Progress (10%)
    await update_job_status(job_id, status="RUNNING", progress=0.1, progress_msg="Lade Trainingsdaten...", update_metrics=True)
    logger.info(f"üìä Job {job_id}: Status auf RUNNING gesetzt, Progress: 10%")
    
    # 4. ‚ö†Ô∏è KRITISCH: Training in run_in_executor ausf√ºhren (CPU-bound!)
    # ‚ö†Ô∏è WICHTIG: W√§hrend des Trainings k√∂nnen wir keine Progress-Updates machen,
    # da es in einem separaten Thread l√§uft. Progress wird nach dem Training aktualisiert.
    # train_model() nutzt bereits intern run_in_executor f√ºr model.fit(),
    # aber sicherheitshalber auch hier nutzen
    logger.info(f"üîÑ Starte Training (blockiert Event Loop nicht)...")
    
    # 5. Update Progress (20% - Training startet)
    await update_job_status(job_id, status="RUNNING", progress=0.2, progress_msg="Training startet...", update_metrics=True)
    logger.info(f"üìä Job {job_id}: Training startet (Progress: 20%)")
    
    # 6. ‚ö†Ô∏è KRITISCH: Speichere urspr√ºnglich angeforderten Features BEVOR features irgendwie modifiziert wird!
    # features kommt direkt aus job['train_features'], das ist die urspr√ºngliche API-Anfrage
    original_requested_features = features.copy() if features else []
    logger.info(f"üîç DEBUG JOB_MANAGER: original_requested_features gespeichert: {len(original_requested_features)} Features: {original_requested_features[:10]}...")
    
    # 7. F√ºhre Training aus (async, nutzt intern run_in_executor)
    logger.info(f"üîÑ Job {job_id}: Starte Training... (kann mehrere Minuten dauern)")
    try:
        training_result = await train_model(
            model_type=model_type,
            features=features,
            target_var=target_var,
            target_operator=target_operator,
            target_value=target_value,
            train_start=train_start,
            train_end=train_end,
            phases=phases,
            params=params,
            model_storage_path=MODEL_STORAGE_PATH,
            # NEU: Zeitbasierte Parameter
            use_time_based=use_time_based,
            future_minutes=future_minutes,
            min_percent_change=min_percent_change,
            direction=direction,
            # ‚ö†Ô∏è FIX: Urspr√ºnglich angeforderten Features direkt aus API-Anfrage √ºbergeben
            original_requested_features=original_requested_features  # NEU: Urspr√ºnglich angeforderten Features
        )
        logger.info(f"‚úÖ Job {job_id}: Training abgeschlossen - Accuracy={training_result['accuracy']:.4f}, F1={training_result['f1']:.4f}")
    except Exception as train_error:
        # Spezifische Training-Fehler direkt hier behandeln
        error_msg = f"Training-Fehler: {str(train_error)}"
        logger.error(f"‚ùå Job {job_id} Training fehlgeschlagen: {error_msg}", exc_info=True)

        # Sofort Status aktualisieren
        await update_job_status(
            job_id,
            status="FAILED",
            progress=0.0,
            error_msg=error_msg
        )

        # Metriken aktualisieren
        update_job_metrics(
            job_id=job_id,
            job_type='TRAIN',
            model_type=model_type,
            status='FAILED',
            progress=0.0,
            duration_seconds=None
        )
        increment_job_counter(job_type='TRAIN', status="failed")

        # Exception weiterwerfen f√ºr globalen Handler
        raise train_error
    
    # 7. Update Progress (60% - Training abgeschlossen)
    await update_job_status(job_id, status="RUNNING", progress=0.6, progress_msg="Training abgeschlossen, speichere Modell...", update_metrics=True)
    logger.info(f"üìä Job {job_id}: Training abgeschlossen (Progress: 60%)")
    
    # 8. Update Progress (80% - Modell wird in DB gespeichert)
    await update_job_status(job_id, status="RUNNING", progress=0.8, progress_msg="Speichere Modell in DB...", update_metrics=True)
    logger.info(f"üìä Job {job_id}: Speichere Modell in DB... (Progress: 80%)")
    
    # 6.5. Hole erweiterte Features aus Training-Result (inkl. engineered features)
    # ‚ö†Ô∏è WICHTIG: Wenn Feature-Engineering aktiviert war, enth√§lt training_result['features'] die erweiterte Liste!
    final_features = training_result.get('features', features)  # Fallback auf urspr√ºngliche Features
    logger.info(f"üìä Features f√ºr Modell-Speicherung: {len(final_features)} (urspr√ºnglich: {len(features)})")
    
    # 7. Erstelle Eintrag in ml_models Tabelle
    # ‚ö†Ô∏è WICHTIG: UTC-Zeitstempel verwenden!
    train_start_dt = train_start
    train_end_dt = train_end
    if isinstance(train_start_dt, str):
        train_start_dt = datetime.fromisoformat(train_start_dt.replace('Z', '+00:00'))
    if isinstance(train_end_dt, str):
        train_end_dt = datetime.fromisoformat(train_end_dt.replace('Z', '+00:00'))
    
    # Stelle sicher, dass Zeitzone UTC ist
    if train_start_dt.tzinfo is None:
        train_start_dt = train_start_dt.replace(tzinfo=timezone.utc)
    if train_end_dt.tzinfo is None:
        train_end_dt = train_end_dt.replace(tzinfo=timezone.utc)
    
    # Hole zeitbasierte Parameter aus Job (falls vorhanden)
    train_future_minutes = job.get('train_future_minutes')
    train_price_change_percent = job.get('train_price_change_percent')
    train_target_direction = job.get('train_target_direction')
    
    model_id = await create_model(
        name=original_model_name,  # ‚ö†Ô∏è Verwende den urspr√ºnglichen Namen!
        model_type=model_type,
        target_variable=target_var,
        train_start=train_start_dt,
        train_end=train_end_dt,
        target_operator=target_operator,
        target_value=target_value,
        features=final_features,  # ‚úÖ Erweiterte Features (inkl. engineered features)
        phases=phases,
        params=params,
        training_accuracy=training_result['accuracy'],
        training_f1=training_result['f1'],
        training_precision=training_result['precision'],
        training_recall=training_result['recall'],
        feature_importance=training_result['feature_importance'],
        model_file_path=training_result['model_path'],
        status="READY",
        cv_scores=training_result.get('cv_scores'),  # NEU: CV-Ergebnisse
        cv_overfitting_gap=training_result.get('cv_overfitting_gap'),  # NEU: Overfitting-Gap
        roc_auc=training_result.get('roc_auc'),  # NEU: ROC-AUC
        mcc=training_result.get('mcc'),  # NEU: MCC
        fpr=training_result.get('fpr'),  # NEU: False Positive Rate
        fnr=training_result.get('fnr'),  # NEU: False Negative Rate
        confusion_matrix=training_result.get('confusion_matrix'),  # NEU: Confusion Matrix
        simulated_profit_pct=training_result.get('simulated_profit_pct'),  # NEU: Simulierter Profit
        # Zeitbasierte Vorhersage-Parameter
        future_minutes=train_future_minutes,
        price_change_percent=train_price_change_percent,
        target_direction=train_target_direction,
        use_flag_features=use_flag_features  # NEU: Flag-Features (bereits aus params extrahiert)
    )
    
    logger.info(f"‚úÖ Job {job_id}: Modell erstellt - ID {model_id}, Name: {original_model_name}")
    
    # 9. Update Progress (90% - Modell gespeichert)
    await update_job_status(job_id, status="RUNNING", progress=0.9, progress_msg="Modell gespeichert, finalisiere...", update_metrics=True)
    logger.info(f"üìä Job {job_id}: Modell gespeichert (Progress: 90%)")
    
    # 10. Setze result_model_id im Job (100% - COMPLETED)
    await update_job_status(
        job_id,
        status="COMPLETED",
        progress=1.0,
        result_model_id=model_id,
        progress_msg=f"Modell {original_model_name} erfolgreich erstellt",
        update_metrics=True
    )
    
    # Erh√∂he Job-Counter
    increment_job_counter('TRAIN', 'COMPLETED')
    
    logger.info(f"üéâ Job {job_id} erfolgreich abgeschlossen: Modell {model_id} erstellt (Progress: 100%)")

async def process_test_job(job: Dict[str, Any]) -> None:
    """
    Verarbeitet TEST Job: Testet Modell auf neuen Daten
    """
    logger.info(f"üß™ Verarbeite TEST Job {job['id']}")
    
    # 1. Hole Job-Parameter
    model_id = job['test_model_id']
    test_start = job['test_start']
    test_end = job['test_end']
    
    logger.info(f"üìã Test-Parameter: Modell {model_id}, Zeitraum: {test_start} bis {test_end}")
    
    # 2. Update Progress
    await update_job_status(job['id'], status="RUNNING", progress=0.2, progress_msg="Lade Test-Daten...")
    
    # 3. F√ºhre Test aus
    test_result = await test_model(
        model_id=model_id,
        test_start=test_start,
        test_end=test_end,
        model_storage_path=MODEL_STORAGE_PATH
    )
    
    logger.info(f"‚úÖ Test abgeschlossen: Accuracy={test_result['accuracy']:.4f}")
    
    # 4. Update Progress
    await update_job_status(job['id'], status="RUNNING", progress=0.8, progress_msg="Speichere Test-Ergebnis...")
    
    # 5. Erstelle Eintrag in ml_test_results Tabelle
    # ‚ö†Ô∏è WICHTIG: UTC-Zeitstempel verwenden!
    test_start_dt = test_start
    test_end_dt = test_end
    if isinstance(test_start_dt, str):
        test_start_dt = datetime.fromisoformat(test_start_dt.replace('Z', '+00:00'))
    if isinstance(test_end_dt, str):
        test_end_dt = datetime.fromisoformat(test_end_dt.replace('Z', '+00:00'))
    
    # Stelle sicher, dass Zeitzone UTC ist
    if test_start_dt.tzinfo is None:
        test_start_dt = test_start_dt.replace(tzinfo=timezone.utc)
    if test_end_dt.tzinfo is None:
        test_end_dt = test_end_dt.replace(tzinfo=timezone.utc)
    
    test_id = await create_test_result(
        model_id=model_id,
        test_start=test_start_dt,
        test_end=test_end_dt,
        accuracy=test_result['accuracy'],
        f1_score=test_result['f1_score'],
        precision_score=test_result['precision_score'],
        recall=test_result['recall'],
        roc_auc=test_result.get('roc_auc'),
        # Zus√§tzliche Metriken (Phase 9)
        mcc=test_result.get('mcc'),
        fpr=test_result.get('fpr'),
        fnr=test_result.get('fnr'),
        simulated_profit_pct=test_result.get('simulated_profit_pct'),
        confusion_matrix=test_result.get('confusion_matrix'),
        # Legacy: Confusion Matrix als einzelne Felder
        tp=test_result['tp'],
        tn=test_result['tn'],
        fp=test_result['fp'],
        fn=test_result['fn'],
        num_samples=test_result['num_samples'],
        num_positive=test_result['num_positive'],
        num_negative=test_result['num_negative'],
        has_overlap=test_result['has_overlap'],
        overlap_note=test_result.get('overlap_note'),
        # Train vs. Test Vergleich (Phase 2)
        train_accuracy=test_result.get('train_accuracy'),
        train_f1=test_result.get('train_f1'),
        train_precision=test_result.get('train_precision'),
        train_recall=test_result.get('train_recall'),
        accuracy_degradation=test_result.get('accuracy_degradation'),
        f1_degradation=test_result.get('f1_degradation'),
        is_overfitted=test_result.get('is_overfitted'),
        # Test-Zeitraum Info (Phase 2)
        test_duration_days=test_result.get('test_duration_days')
    )
    
    logger.info(f"‚úÖ Test-Ergebnis erstellt: ID {test_id}")
    
    # 6. Setze result_test_id im Job
    await update_job_status(
        job['id'],
        status="COMPLETED",
        progress=1.0,
        result_test_id=test_id,
        progress_msg=f"Test erfolgreich abgeschlossen"
    )
    
    logger.info(f"üéâ TEST Job {job['id']} erfolgreich: Test {test_id} erstellt")

async def process_compare_job(job: Dict[str, Any]) -> None:
    """
    Verarbeitet COMPARE Job: Vergleicht bis zu 4 Modelle
    
    ‚úÖ NEUE STRUKTUR (v2):
    - Unterst√ºtzt 2-4 Modelle
    - Berechnet Durchschnitts-Score f√ºr Ranking
    - Speichert alle Ergebnisse als JSONB
    """
    logger.info(f"‚öñÔ∏è Verarbeite COMPARE Job {job['id']}")
    
    # 1. Hole Job-Parameter (unterst√ºtzt beide Formate)
    model_ids = job.get('compare_model_ids')
    if not model_ids:
        # Legacy-Support: model_a_id und model_b_id
        model_a_id = job.get('compare_model_a_id')
        model_b_id = job.get('compare_model_b_id')
        if model_a_id and model_b_id:
            model_ids = [model_a_id, model_b_id]
        else:
            raise ValueError("Keine Modell-IDs f√ºr Vergleich gefunden!")
    
    test_start = job['compare_start']
    test_end = job['compare_end']
    
    logger.info(f"üìã Vergleich: {len(model_ids)} Modelle - {model_ids}")
    
    # 2. Konvertiere Zeitstempel
    test_start_dt = test_start
    test_end_dt = test_end
    if isinstance(test_start_dt, str):
        test_start_dt = datetime.fromisoformat(test_start_dt.replace('Z', '+00:00'))
    if isinstance(test_end_dt, str):
        test_end_dt = datetime.fromisoformat(test_end_dt.replace('Z', '+00:00'))
    if test_start_dt.tzinfo is None:
        test_start_dt = test_start_dt.replace(tzinfo=timezone.utc)
    if test_end_dt.tzinfo is None:
        test_end_dt = test_end_dt.replace(tzinfo=timezone.utc)
    
    # 3. Teste alle Modelle
    results = []
    test_result_ids = []
    num_models = len(model_ids)
    
    for i, model_id in enumerate(model_ids):
        progress = 0.1 + (i / num_models) * 0.7  # 10% - 80%
        await update_job_status(
            job['id'], 
            status="RUNNING", 
            progress=progress, 
            progress_msg=f"Teste Modell {i+1}/{num_models} (ID: {model_id})..."
        )
        
        # Teste Modell
        result = await test_model(
            model_id=model_id,
            test_start=test_start,
            test_end=test_end,
            model_storage_path=MODEL_STORAGE_PATH
        )
        
        logger.info(f"‚úÖ Modell {model_id}: Accuracy={result['accuracy']:.4f}, F1={result['f1_score']:.4f}")
        
        # Erstelle/finde Test-Ergebnis in DB
        test_id = await get_or_create_test_result(
            model_id=model_id,
            test_start=test_start_dt,
            test_end=test_end_dt,
            accuracy=result['accuracy'],
            f1_score=result['f1_score'],
            precision_score=result['precision_score'],
            recall=result['recall'],
            roc_auc=result.get('roc_auc'),
            mcc=result.get('mcc'),
            fpr=result.get('fpr'),
            fnr=result.get('fnr'),
            simulated_profit_pct=result.get('simulated_profit_pct'),
            confusion_matrix=result.get('confusion_matrix'),
            tp=result['tp'],
            tn=result['tn'],
            fp=result['fp'],
            fn=result['fn'],
            num_samples=result['num_samples'],
            num_positive=result['num_positive'],
            num_negative=result['num_negative'],
            has_overlap=result['has_overlap'],
            overlap_note=result.get('overlap_note'),
            train_accuracy=result.get('train_accuracy'),
            train_f1=result.get('train_f1'),
            train_precision=result.get('train_precision'),
            train_recall=result.get('train_recall'),
            accuracy_degradation=result.get('accuracy_degradation'),
            f1_degradation=result.get('f1_degradation'),
            is_overfitted=result.get('is_overfitted'),
            test_duration_days=result.get('test_duration_days')
        )
        
        logger.info(f"‚úÖ Test-Ergebnis f√ºr Modell {model_id}: ID {test_id}")
        test_result_ids.append(test_id)
        
        # Berechne Durchschnitts-Score (Accuracy, F1, Profit normalisiert)
        accuracy = result['accuracy'] or 0
        f1 = result['f1_score'] or 0
        profit = result.get('simulated_profit_pct') or 0
        # Normalisiere Profit auf 0-1 Skala (angenommen max ¬±10%)
        profit_normalized = max(0, min(1, (profit + 10) / 20))
        avg_score = (accuracy + f1 + profit_normalized) / 3
        
        results.append({
            'model_id': model_id,
            'test_result_id': test_id,
            'accuracy': accuracy,
            'f1_score': f1,
            'precision': result['precision_score'] or 0,
            'recall': result['recall'] or 0,
            'roc_auc': result.get('roc_auc') or 0,
            'mcc': result.get('mcc') or 0,
            'fpr': result.get('fpr') or 0,
            'fnr': result.get('fnr') or 0,
            'simulated_profit_pct': profit,
            'confusion_matrix': result.get('confusion_matrix'),
            'avg_score': avg_score
        })
    
    # 4. Sortiere nach Durchschnitts-Score und weise R√§nge zu
    results.sort(key=lambda x: x['avg_score'], reverse=True)
    for rank, r in enumerate(results, 1):
        r['rank'] = rank
    
    # 5. Bestimme Gewinner
    winner = results[0]
    winner_id = winner['model_id']
    winner_reason = f"Bester Durchschnitt (Acc: {winner['accuracy']:.2%}, F1: {winner['f1_score']:.2%}, Profit: {winner['simulated_profit_pct']:.2f}%) = Score: {winner['avg_score']:.4f}"
    
    logger.info(f"üèÜ Gewinner: Modell {winner_id} - {winner_reason}")
    
    # 6. Update Progress
    await update_job_status(job['id'], status="RUNNING", progress=0.9, progress_msg="Speichere Vergleich...")
    
    # 7. Erstelle Eintrag in ml_comparisons Tabelle
    comparison_id = await create_comparison_v2(
        model_ids=model_ids,
        test_result_ids=test_result_ids,
        results=results,
        test_start=test_start_dt,
        test_end=test_end_dt,
        num_samples=results[0].get('num_samples') if results else None,
        winner_id=winner_id,
        winner_reason=winner_reason
    )
    
    logger.info(f"‚úÖ Vergleich erstellt: ID {comparison_id}")
    
    # 8. Setze result_comparison_id im Job
    await update_job_status(
        job['id'],
        status="COMPLETED",
        progress=1.0,
        result_comparison_id=comparison_id,
        progress_msg=f"Vergleich erfolgreich: {len(model_ids)} Modelle verglichen"
    )
    
    logger.info(f"üéâ COMPARE Job {job['id']} erfolgreich: Vergleich {comparison_id} erstellt")

async def start_worker() -> None:
    """
    Worker-Loop: Pr√ºft regelm√§√üig auf neue Jobs und verarbeitet sie
    
    ‚ö†Ô∏è WICHTIG: L√§uft in Endlosschleife, pr√ºft alle JOB_POLL_INTERVAL Sekunden
    """
    logger.info(f"üîÑ Starte Job Worker (Poll-Interval: {JOB_POLL_INTERVAL}s, Max Concurrent: {MAX_CONCURRENT_JOBS})")
    
    active_tasks = set()  # Track laufende Jobs
    
    while True:
        try:
            # Pr√ºfe ob noch Platz f√ºr neue Jobs
            if len(active_tasks) < MAX_CONCURRENT_JOBS:
                # Hole n√§chsten PENDING Job
                job = await get_next_pending_job()
                
                if job:
                    logger.info(f"üì• Neuer Job gefunden: {job['id']} ({job['job_type']})")
                    
                    # Starte Job asynchron (nicht blockierend!)
                    task = asyncio.create_task(process_job(job['id']))
                    active_tasks.add(task)
                    
                    # Entferne Task wenn fertig
                    def remove_task(t):
                        active_tasks.discard(t)
                    task.add_done_callback(remove_task)
                else:
                    # Kein Job gefunden, warte
                    await asyncio.sleep(JOB_POLL_INTERVAL)
            else:
                # Max Concurrent Jobs erreicht, warte
                logger.debug(f"‚è≥ Max Concurrent Jobs erreicht ({MAX_CONCURRENT_JOBS}), warte...")
                await asyncio.sleep(JOB_POLL_INTERVAL)
            
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Worker Error: {e}")
            import traceback
            traceback.print_exc()
            await asyncio.sleep(JOB_POLL_INTERVAL)

