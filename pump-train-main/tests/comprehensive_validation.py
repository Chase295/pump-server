#!/usr/bin/env python3
"""
Umfassende Validierung aller Daten - Pr√ºft ob alle Werte korrekt sind
und keine erfundenen Daten angezeigt werden
"""
import asyncio
import httpx
import json
import os
from typing import Dict, Any, List
from datetime import datetime

API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8000/api")

async def comprehensive_validation():
    """Umfassende Validierung aller Daten"""
    print("="*80)
    print("üîç UMFASSENDE VALIDIERUNG ALLER DATEN")
    print("="*80)
    print()
    
    async with httpx.AsyncClient(timeout=60.0) as client:
        errors = []
        warnings = []
        
        # 1. Lade alle Modelle
        print("1Ô∏è‚É£ Lade alle Modelle...")
        try:
            response = await client.get(f"{API_BASE_URL}/models")
            response.raise_for_status()
            models = response.json()
            print(f"   ‚úÖ {len(models)} Modelle gefunden")
        except Exception as e:
            errors.append(f"‚ùå Fehler beim Laden der Modelle: {e}")
            return
        
        # 2. Lade alle Tests
        print("\n2Ô∏è‚É£ Lade alle Test-Ergebnisse...")
        try:
            response = await client.get(f"{API_BASE_URL}/test-results")
            response.raise_for_status()
            tests = response.json()
            print(f"   ‚úÖ {len(tests)} Test-Ergebnisse gefunden")
        except Exception as e:
            errors.append(f"‚ùå Fehler beim Laden der Tests: {e}")
            return
        
        # 3. Lade alle Vergleiche
        print("\n3Ô∏è‚É£ Lade alle Vergleiche...")
        try:
            response = await client.get(f"{API_BASE_URL}/comparisons")
            response.raise_for_status()
            comparisons = response.json()
            print(f"   ‚úÖ {len(comparisons)} Vergleiche gefunden")
        except Exception as e:
            errors.append(f"‚ùå Fehler beim Laden der Vergleiche: {e}")
            return
        
        # 4. Validiere jedes Modell
        print("\n" + "="*80)
        print("4Ô∏è‚É£ VALIDIERE JEDES MODELL")
        print("="*80)
        for model in models:
            model_id = model.get('id')
            print(f"\nüìä Modell {model_id}: {model.get('name', 'N/A')}")
            
            # Pr√ºfe Basis-Felder
            required_fields = ['id', 'name', 'model_type', 'status', 'train_start', 'train_end']
            for field in required_fields:
                if field not in model or model[field] is None:
                    errors.append(f"‚ùå Modell {model_id}: Feld '{field}' fehlt")
            
            # Pr√ºfe Status
            if model.get('status') != 'READY':
                warnings.append(f"‚ö†Ô∏è Modell {model_id}: Status ist nicht READY: {model.get('status')}")
            
            # Pr√ºfe Confusion Matrix
            cm = model.get('confusion_matrix')
            tp = model.get('tp', 0)
            tn = model.get('tn', 0)
            fp = model.get('fp', 0)
            fn = model.get('fn', 0)
            
            if cm:
                if cm.get('tp') != tp or cm.get('tn') != tn or cm.get('fp') != fp or cm.get('fn') != fn:
                    errors.append(f"‚ùå Modell {model_id}: Confusion Matrix Werte stimmen nicht √ºberein!")
            
            # Pr√ºfe Accuracy-Berechnung
            total = tp + tn + fp + fn
            if total > 0:
                calculated_accuracy = (tp + tn) / total
                stored_accuracy = model.get('training_accuracy')
                if stored_accuracy and abs(calculated_accuracy - stored_accuracy) > 0.001:
                    errors.append(f"‚ùå Modell {model_id}: Accuracy stimmt nicht! Berechnet: {calculated_accuracy:.4f}, Gespeichert: {stored_accuracy:.4f}")
            
            # Pr√ºfe Feature Importance Summe
            feature_importance = model.get('feature_importance')
            if feature_importance:
                importance_sum = sum(feature_importance.values())
                if abs(importance_sum - 1.0) > 0.01:
                    errors.append(f"‚ùå Modell {model_id}: Feature Importance summiert nicht zu 1.0: {importance_sum:.4f}")
            
            # Pr√ºfe Zeitstempel
            train_start = model.get('train_start')
            train_end = model.get('train_end')
            if train_start and train_end:
                try:
                    start_dt = datetime.fromisoformat(train_start.replace('Z', '+00:00'))
                    end_dt = datetime.fromisoformat(train_end.replace('Z', '+00:00'))
                    if end_dt <= start_dt:
                        errors.append(f"‚ùå Modell {model_id}: train_end muss nach train_start sein!")
                except Exception as e:
                    errors.append(f"‚ùå Modell {model_id}: Ung√ºltige Zeitstempel: {e}")
        
        # 5. Validiere jeden Test
        print("\n" + "="*80)
        print("5Ô∏è‚É£ VALIDIERE JEDEN TEST")
        print("="*80)
        for test in tests:
            test_id = test.get('id')
            model_id = test.get('model_id')
            print(f"\nüìä Test {test_id}: Modell {model_id}")
            
            # Pr√ºfe ob Modell existiert
            model_exists = any(m.get('id') == model_id for m in models)
            if not model_exists:
                errors.append(f"‚ùå Test {test_id}: Modell {model_id} existiert nicht!")
            
            # Pr√ºfe Confusion Matrix
            cm = test.get('confusion_matrix')
            tp = test.get('tp', 0)
            tn = test.get('tn', 0)
            fp = test.get('fp', 0)
            fn = test.get('fn', 0)
            num_samples = test.get('num_samples', 0)
            
            if cm:
                if cm.get('tp') != tp or cm.get('tn') != tn or cm.get('fp') != fp or cm.get('fn') != fn:
                    errors.append(f"‚ùå Test {test_id}: Confusion Matrix Werte stimmen nicht √ºberein!")
            
            # Pr√ºfe Summe
            total = tp + tn + fp + fn
            if total != num_samples:
                errors.append(f"‚ùå Test {test_id}: Confusion Matrix summiert nicht zu num_samples! {total} != {num_samples}")
            
            # Pr√ºfe Accuracy-Berechnung
            if total > 0:
                calculated_accuracy = (tp + tn) / total
                stored_accuracy = test.get('accuracy')
                if stored_accuracy and abs(calculated_accuracy - stored_accuracy) > 0.001:
                    errors.append(f"‚ùå Test {test_id}: Accuracy stimmt nicht! Berechnet: {calculated_accuracy:.4f}, Gespeichert: {stored_accuracy:.4f}")
            
            # Pr√ºfe Label-Balance
            num_positive = test.get('num_positive', 0)
            num_negative = test.get('num_negative', 0)
            if num_positive + num_negative != num_samples:
                errors.append(f"‚ùå Test {test_id}: Label-Balance stimmt nicht! {num_positive} + {num_negative} != {num_samples}")
            
            # Pr√ºfe Zeitstempel
            test_start = test.get('test_start')
            test_end = test.get('test_end')
            if test_start and test_end:
                try:
                    start_dt = datetime.fromisoformat(test_start.replace('Z', '+00:00'))
                    end_dt = datetime.fromisoformat(test_end.replace('Z', '+00:00'))
                    if end_dt <= start_dt:
                        errors.append(f"‚ùå Test {test_id}: test_end muss nach test_start sein!")
                except Exception as e:
                    errors.append(f"‚ùå Test {test_id}: Ung√ºltige Zeitstempel: {e}")
            
            # Pr√ºfe Train vs. Test Vergleich
            train_accuracy = test.get('train_accuracy')
            test_accuracy = test.get('accuracy')
            accuracy_degradation = test.get('accuracy_degradation')
            
            if train_accuracy and test_accuracy and accuracy_degradation is not None:
                calculated_degradation = train_accuracy - test_accuracy
                if abs(calculated_degradation - accuracy_degradation) > 0.001:
                    errors.append(f"‚ùå Test {test_id}: Accuracy Degradation stimmt nicht! Berechnet: {calculated_degradation:.4f}, Gespeichert: {accuracy_degradation:.4f}")
        
        # 6. Validiere jeden Vergleich
        print("\n" + "="*80)
        print("6Ô∏è‚É£ VALIDIERE JEDEN VERGLEICH")
        print("="*80)
        for comparison in comparisons:
            comp_id = comparison.get('id')
            model_a_id = comparison.get('model_a_id')
            model_b_id = comparison.get('model_b_id')
            test_a_id = comparison.get('test_a_id')
            test_b_id = comparison.get('test_b_id')
            
            print(f"\nüìä Vergleich {comp_id}: Modell A={model_a_id}, Modell B={model_b_id}")
            
            # Pr√ºfe ob Modelle existieren
            model_a_exists = any(m.get('id') == model_a_id for m in models)
            model_b_exists = any(m.get('id') == model_b_id for m in models)
            if not model_a_exists:
                errors.append(f"‚ùå Vergleich {comp_id}: Modell A {model_a_id} existiert nicht!")
            if not model_b_exists:
                errors.append(f"‚ùå Vergleich {comp_id}: Modell B {model_b_id} existiert nicht!")
            
            # Pr√ºfe ob Tests existieren
            if test_a_id:
                test_a_exists = any(t.get('id') == test_a_id for t in tests)
                if not test_a_exists:
                    errors.append(f"‚ùå Vergleich {comp_id}: Test A {test_a_id} existiert nicht!")
                else:
                    # Pr√ºfe ob Test A zum Modell A geh√∂rt
                    test_a = next(t for t in tests if t.get('id') == test_a_id)
                    if test_a.get('model_id') != model_a_id:
                        errors.append(f"‚ùå Vergleich {comp_id}: Test A {test_a_id} geh√∂rt nicht zu Modell A {model_a_id}!")
            
            if test_b_id:
                test_b_exists = any(t.get('id') == test_b_id for t in tests)
                if not test_b_exists:
                    errors.append(f"‚ùå Vergleich {comp_id}: Test B {test_b_id} existiert nicht!")
                else:
                    # Pr√ºfe ob Test B zum Modell B geh√∂rt
                    test_b = next(t for t in tests if t.get('id') == test_b_id)
                    if test_b.get('model_id') != model_b_id:
                        errors.append(f"‚ùå Vergleich {comp_id}: Test B {test_b_id} geh√∂rt nicht zu Modell B {model_b_id}!")
            
            # Pr√ºfe Winner
            winner_id = comparison.get('winner_id')
            if winner_id and winner_id not in [model_a_id, model_b_id]:
                errors.append(f"‚ùå Vergleich {comp_id}: Winner {winner_id} ist weder Modell A noch B!")
        
        # 7. Zusammenfassung
        print("\n" + "="*80)
        print("üìä VALIDIERUNGS-ERGEBNIS")
        print("="*80)
        print(f"\n‚úÖ Modelle gepr√ºft: {len(models)}")
        print(f"‚úÖ Tests gepr√ºft: {len(tests)}")
        print(f"‚úÖ Vergleiche gepr√ºft: {len(comparisons)}")
        
        if errors:
            print(f"\n‚ùå FEHLER GEFUNDEN: {len(errors)}")
            for error in errors:
                print(f"   {error}")
        else:
            print("\n‚úÖ KEINE FEHLER GEFUNDEN!")
        
        if warnings:
            print(f"\n‚ö†Ô∏è WARNUNGEN: {len(warnings)}")
            for warning in warnings:
                print(f"   {warning}")
        else:
            print("\n‚úÖ KEINE WARNUNGEN!")
        
        print("\n" + "="*80)
        
        if errors:
            return False
        return True

if __name__ == "__main__":
    success = asyncio.run(comprehensive_validation())
    exit(0 if success else 1)

