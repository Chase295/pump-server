"""
Feature Engineering f√ºr ML Training Service
L√§dt Daten aus coin_metrics und erstellt Labels
"""
import pandas as pd
import numpy as np
import logging
from typing import List, Optional, Dict, Any
from datetime import datetime, timezone
from app.database.connection import get_pool

logger = logging.getLogger(__name__)

# ‚ö†Ô∏è RAM-Management: Max Anzahl Zeilen (deaktiviert f√ºr Stabilit√§t)
# MAX_TRAINING_ROWS = 500000  # Deaktiviert wegen Parameter-Fehlern

def _ensure_utc(dt: str | datetime) -> datetime:
    """
    Konvertiert datetime zu UTC (tz-aware).
    
    Hilfsfunktion f√ºr konsistente Zeitzone-Behandlung.
    Unterst√ºtzt ISO-Format Strings und datetime-Objekte.
    
    Args:
        dt: Datetime als String (ISO-Format) oder datetime-Objekt
        
    Returns:
        datetime-Objekt mit UTC-Zeitzone
        
    Example:
        ```python
        dt1 = _ensure_utc("2024-01-01T00:00:00Z")
        dt2 = _ensure_utc(datetime.now())
        ```
    """
    if isinstance(dt, str):
        # ISO-Format mit Z oder +00:00
        dt = dt.replace('Z', '+00:00')
        dt = datetime.fromisoformat(dt)
    
    if dt.tzinfo is None:
        # Keine Zeitzone ‚Üí UTC annehmen
        dt = dt.replace(tzinfo=timezone.utc)
    else:
        # Konvertiere zu UTC falls andere Zeitzone
        dt = dt.astimezone(timezone.utc)
    
    return dt

async def load_training_data(
    train_start: str | datetime,
    train_end: str | datetime,
    features: List[str],
    phases: Optional[List[int]] = None,
    include_ath: bool = True  # NEU: ATH-Daten optional laden
) -> pd.DataFrame:
    """
    L√§dt Trainingsdaten aus coin_metrics
    
    ‚ö†Ô∏è KRITISCH: UTC-Zeitzone!
    ‚ö†Ô∏è RAM-Management: LIMIT 500000 Zeilen!
    
    Args:
        train_start: Start-Zeitpunkt (ISO-Format oder datetime, wird zu UTC konvertiert)
        train_end: Ende-Zeitpunkt (ISO-Format oder datetime, wird zu UTC konvertiert)
        features: Liste der Feature-Namen (z.B. ["price_open", "price_high", "volume_sol"])
        phases: Liste der Coin-Phasen (z.B. [1, 2, 3]) oder None f√ºr alle
        include_ath: Wenn True, werden ATH-Daten aus coin_streams geladen (Default: True)
    
    Returns:
        DataFrame mit Trainingsdaten
    """
    pool = await get_pool()
    
    # ‚ö†Ô∏è Konvertiere zu UTC
    train_start_utc = _ensure_utc(train_start)
    train_end_utc = _ensure_utc(train_end)
    
    # ‚ö†Ô∏è WICHTIG: Lade IMMER alle neuen Metriken (auch wenn nicht in Features-Liste)
    # Damit sind sie f√ºr Feature-Engineering verf√ºgbar
    base_columns = """
        cm.timestamp, 
        cm.phase_id_at_time,
        cm.mint,
        
        -- Basis OHLC
        cm.price_open, cm.price_high, cm.price_low, cm.price_close,
        
        -- Volumen
        cm.volume_sol, cm.buy_volume_sol, cm.sell_volume_sol, cm.net_volume_sol,
        
        -- Market Cap & Phase
        cm.market_cap_close,
        
        -- ‚ö†Ô∏è KRITISCH: Dev-Tracking (Rug-Pull-Indikator)
        cm.dev_sold_amount,
        
        -- Ratio-Metriken (Bot-Spam vs. echtes Interesse)
        cm.buy_pressure_ratio,
        cm.unique_signer_ratio,
        
        -- Whale-Aktivit√§t
        cm.whale_buy_volume_sol,
        cm.whale_sell_volume_sol,
        cm.num_whale_buys,
        cm.num_whale_sells,
        
        -- Volatilit√§t
        cm.volatility_pct,
        cm.avg_trade_size_sol
    """
    
    # üÜï ATH-Tracking (vereinfacht - historische Berechnung erfolgt in Python)
    if include_ath:
        base_columns += """,
            -- ATH-Daten aus coin_streams (nur aktuelle Werte f√ºr historische Berechnung)
            COALESCE(cs.ath_price_sol, cm.price_high) as current_ath_price_sol,
            cs.ath_timestamp as current_ath_timestamp
        """
    
    # Zus√§tzliche Features aus Request (falls nicht bereits in base_columns)
    base_column_names = [
        'timestamp', 'phase_id_at_time', 'mint', 'price_open', 'price_high', 'price_low', 'price_close',
        'volume_sol', 'buy_volume_sol', 'sell_volume_sol', 'net_volume_sol',
        'market_cap_close', 'dev_sold_amount', 'buy_pressure_ratio', 'unique_signer_ratio',
        'whale_buy_volume_sol', 'whale_sell_volume_sol', 'num_whale_buys', 'num_whale_sells',
        'volatility_pct', 'avg_trade_size_sol'
    ]
    
    # üÜï ATH-Features hinzuf√ºgen (wenn include_ath aktiviert)
    if include_ath:
        base_column_names.extend([
            'current_ath_price_sol', 'current_ath_timestamp'
        ])
    
    # ‚ö†Ô∏è HINWEIS: Engineered Features sind bereits in der Datenbank gespeichert
    # Lade alle verf√ºgbaren Features aus der DB
    logger.info(f"‚ÑπÔ∏è Lade {len(features)} Features aus Datenbank (inkl. engineered)")
    filtered_features = features.copy()

    additional_features = [f for f in filtered_features if f not in base_column_names]
    
    # Phase-Filter
    if phases:
        phase_filter = "AND phase_id_at_time = ANY($3)"
        params = [train_start_utc, train_end_utc, phases]
        param_count = 3
    else:
        phase_filter = ""
        params = [train_start_utc, train_end_utc]
        param_count = 2
    
    # üÜï JOIN mit coin_streams f√ºr ATH-Daten (wenn aktiviert)
    if include_ath:
        join_clause = """
            LEFT JOIN coin_streams cs ON cm.mint = cs.token_address
        """
    else:
        join_clause = ""
    
    # üõ†Ô∏è MAXIMAL EINFACHE L√ñSUNG: Kein LIMIT f√ºr ersten Test
    logger.info(f"üéØ Verwende einfache Query ohne LIMIT f√ºr ersten Test")

    # Einfachste m√∂gliche Query
    if additional_features:
        additional_list = ", ".join(additional_features)
        query = f"""
            SELECT {base_columns}, {additional_list}
            FROM coin_metrics cm
            {join_clause}
            WHERE cm.timestamp >= $1 AND cm.timestamp <= $2
            {phase_filter}
            ORDER BY cm.timestamp
        """
    else:
        query = f"""
            SELECT {base_columns}
            FROM coin_metrics cm
            {join_clause}
            WHERE cm.timestamp >= $1 AND cm.timestamp <= $2
            {phase_filter}
            ORDER BY cm.timestamp
        """
    
    ath_status = "mit ATH-Daten" if include_ath else "ohne ATH-Daten"
    logger.info(f"üìä Lade Daten: {train_start_utc} bis {train_end_utc}, Features: {features}, Phasen: {phases}, {ath_status}")
    logger.info(f"üöÄ Skalierbare Datenverarbeitung aktiviert")

    # üõ†Ô∏è ULTRA-STABILE DATENLADUNG
    logger.info("üì¶ Ultra-stabile Datenladung ohne Chunking")

    # Parameter-Setup: Stelle sicher, dass nur datetime-Parameter verwendet werden
    query_params = [train_start_utc, train_end_utc]
    if phases:
        query_params.append(phases)

    # Entferne jegliche LIMIT-Referenzen aus der Query
    clean_query = query.replace(" LIMIT $3", "").replace(" LIMIT $4", "").replace("LIMIT $3", "").replace("LIMIT $4", "")

    try:
        print(f"üîß DEBUG: Query = {clean_query}")
        print(f"üîß DEBUG: Params = {query_params}")
        print(f"üîß DEBUG: Param types = {[type(p) for p in query_params]}")

        rows = await pool.fetch(clean_query, *query_params)
        logger.info(f"‚úÖ Daten geladen: {len(rows)} Zeilen mit {len(query_params)} Parametern")

    except Exception as e:
        logger.error(f"‚ùå Fehler bei Datenladung: {e}")
        print(f"üîß DEBUG ERROR: {e}")
        print(f"üîß DEBUG Query: {clean_query}")
        print(f"üîß DEBUG Params: {query_params}")
        rows = []
    
    if not rows:
        logger.warning("‚ö†Ô∏è Keine Daten gefunden!")
        return pd.DataFrame()
    
    # Konvertiere zu DataFrame
    data = pd.DataFrame([dict(row) for row in rows])

    # üßπ DATA CLEANING: Entferne "tote" Daten (Garbage In, Garbage Out verhindern)
    # 1. Entferne Zeilen mit NULL-Werten in kritischen Spalten
    # Diese Coins hatten nie Trades oder wurden vom Tracker verpasst
    logger.info(f"üßπ Data Cleaning: {len(data)} Zeilen vor Filter")

    # Kritische Spalten, die nicht NULL sein d√ºrfen
    critical_columns = ['price_close', 'volume_sol']
    if include_ath:
        critical_columns.append('current_ath_price_sol')

    # Entferne Zeilen mit NULL in kritischen Spalten
    data_clean = data.dropna(subset=critical_columns)
    logger.info(f"üßπ Data Cleaning: {len(data_clean)} Zeilen nach NULL-Filter (entfernt: {len(data) - len(data_clean)})")

    # 2. Entferne Coins mit zu wenigen Datenpunkten (Rauschen herausfiltern)
    # Ein Coin mit nur 3 Datenpunkten kann kein sinnvolles Muster zeigen
    if 'mint' in data_clean.columns:
        coin_counts = data_clean['mint'].value_counts()
        valid_mints = coin_counts[coin_counts >= 30].index  # Mindestens 30 Eintr√§ge pro Coin
        data_final = data_clean[data_clean['mint'].isin(valid_mints)]
        removed_coins = len(coin_counts) - len(valid_mints)
        logger.info(f"üßπ Data Cleaning: {len(data_final)} Zeilen nach Coin-Filter (entfernt: {removed_coins} Coins mit <30 Datenpunkten)")
    else:
        data_final = data_clean
        logger.info("üßπ Data Cleaning: Kein 'mint' Column gefunden, √ºberspringe Coin-Filter")

    # Verwende die bereinigten Daten weiter
    data = data_final

    # ‚ö†Ô∏è WICHTIG: Konvertiere alle Decimal-Typen zu float (PostgreSQL liefert Decimal)
    # Dies verhindert "unsupported operand type(s) for -: 'decimal.Decimal' and 'float'" Fehler
    for col in data.columns:
        if col != 'timestamp' and col != 'phase_id_at_time':  # Timestamp und Phase-ID bleiben unver√§ndert
            # Konvertiere zu numeric (float), ignoriere Fehler (z.B. bei Strings)
            data[col] = pd.to_numeric(data[col], errors='coerce')

    # üÜï HISTORISCHE ATH-FEATURES: Data Leakage-frei berechnen
    # ‚ö†Ô∏è WICHTIG: ATH-Features werden jetzt in create_pump_detection_features() hinzugef√ºgt
    # Nicht mehr hier, um Konflikte zu vermeiden
    if include_ath:
        logger.info("‚ÑπÔ∏è ATH-Daten werden sp√§ter in create_pump_detection_features() berechnet")
            # Bei Fehlern: Verwende Original-Daten ohne ATH-Features

    # Setze timestamp als Index
    if 'timestamp' in data.columns:
        # ‚ö†Ô∏è WICHTIG: Entferne doppelte Timestamps (kann bei mehreren Coins passieren)
        # Behalte nur die erste Zeile pro Timestamp
        data = data.drop_duplicates(subset='timestamp', keep='first')
        data.set_index('timestamp', inplace=True)
        # Sortiere nach Index (falls nicht bereits sortiert)
        data = data.sort_index()

    # ‚ö†Ô∏è WICHTIG: phase_id_at_time muss als Spalte bleiben (f√ºr zeitbasierte Labels)
    # Es wird nicht als Index verwendet, sondern als normale Spalte behalten
    
    logger.info(f"‚úÖ {len(data)} Zeilen final (nach Data Cleaning und Duplikat-Entfernung)")
    
    # Pr√ºfe ob LIMIT erreicht wurde (vor Data Cleaning)
    # Nach Data Cleaning kann die Anzahl geringer sein
    if len(data) == 0:
        logger.warning("‚ö†Ô∏è Alle Daten wurden durch Data Cleaning entfernt! √úberpr√ºfe die Datenbank.")
    elif len(data) < 1000:
        logger.warning(f"‚ö†Ô∏è Sehr wenige Daten nach Cleaning: {len(data)} Zeilen. Modell-Training k√∂nnte unzuverl√§ssig sein.")
    
    return data

async def enrich_with_market_context(
    data: pd.DataFrame,
    train_start: datetime,
    train_end: datetime
) -> pd.DataFrame:
    """
    F√ºgt Marktstimmung (SOL-Preis) zu Trainingsdaten hinzu.
    Merge mit Forward-Fill (nimmt letzten bekannten Wert).
    
    Args:
        data: DataFrame mit Trainingsdaten (muss timestamp als Index haben)
        train_start: Start-Zeitpunkt
        train_end: Ende-Zeitpunkt
    
    Returns:
        DataFrame mit zus√§tzlichen Spalten: sol_price_usd, sol_price_change_pct, sol_price_ma_5, sol_price_volatility
    """
    pool = await get_pool()
    
    # Konvertiere zu UTC
    train_start_utc = _ensure_utc(train_start)
    train_end_utc = _ensure_utc(train_end)
    
    # Lade Exchange Rates
    sql = """
        SELECT 
            created_at as timestamp,
            sol_price_usd,
            usd_to_eur_rate
        FROM exchange_rates
        WHERE created_at >= $1 AND created_at <= $2
        ORDER BY created_at
    """
    
    try:
        rows = await pool.fetch(sql, train_start_utc, train_end_utc)
        
        if not rows:
            logger.warning("‚ö†Ô∏è Keine Exchange Rates gefunden - Marktstimmung wird nicht hinzugef√ºgt")
            return data
        
        # Konvertiere zu DataFrame
        rates_df = pd.DataFrame([dict(row) for row in rows])
        rates_df['timestamp'] = pd.to_datetime(rates_df['timestamp'])
        rates_df.set_index('timestamp', inplace=True)
        
        # Merge mit Forward-Fill (nehme letzten bekannten Wert)
        data = data.merge(rates_df[['sol_price_usd']], left_index=True, right_index=True, how='left')
        data['sol_price_usd'].fillna(method='ffill', inplace=True)
        
        # ‚úÖ NEUE Features berechnen:
        data['sol_price_change_pct'] = data['sol_price_usd'].pct_change() * 100
        data['sol_price_ma_5'] = data['sol_price_usd'].rolling(5, min_periods=1).mean()
        data['sol_price_volatility'] = data['sol_price_usd'].rolling(10, min_periods=1).std()
        
        # NaN-Werte durch 0 ersetzen (am Anfang der Serie)
        data['sol_price_change_pct'].fillna(0, inplace=True)
        data['sol_price_ma_5'].fillna(data['sol_price_usd'], inplace=True)
        data['sol_price_volatility'].fillna(0, inplace=True)
        
        logger.info("‚úÖ Marktstimmung hinzugef√ºgt: sol_price_usd, sol_price_change_pct, sol_price_ma_5, sol_price_volatility")
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Fehler beim Laden der Exchange Rates: {e} - Marktstimmung wird nicht hinzugef√ºgt")
    
    return data

def create_labels(
    data: pd.DataFrame,
    target_variable: str,
    target_operator: str,
    target_value: float
) -> pd.Series:
    """
    Erstellt bin√§re Labels (0/1) basierend auf target_variable/operator/value
    
    Args:
        data: DataFrame mit Trainingsdaten
        target_variable: Ziel-Variable (z.B. "market_cap_close")
        target_operator: Vergleichsoperator (">", "<", ">=", "<=", "=")
        target_value: Schwellwert
    
    Returns:
        Series mit bin√§ren Labels (0 oder 1)
    """
    if target_variable not in data.columns:
        raise ValueError(f"Target-Variable '{target_variable}' nicht in Daten gefunden!")
    
    values = data[target_variable]
    
    # Erstelle Labels basierend auf Operator
    if target_operator == ">":
        labels = (values > target_value).astype(int)
    elif target_operator == "<":
        labels = (values < target_value).astype(int)
    elif target_operator == ">=":
        labels = (values >= target_value).astype(int)
    elif target_operator == "<=":
        labels = (values <= target_value).astype(int)
    elif target_operator == "=":
        labels = (values == target_value).astype(int)
    else:
        raise ValueError(f"Unbekannter Operator: {target_operator}")
    
    positive = labels.sum()
    negative = len(labels) - positive
    
    logger.info(f"‚úÖ Labels erstellt: {positive} positive, {negative} negative")
    
    return labels

def create_time_based_labels(
    data: pd.DataFrame,
    target_variable: str,
    future_minutes: int,
    min_percent_change: float,
    direction: str = "up",  # "up" oder "down"
    phase_intervals: Optional[Dict[int, int]] = None  # {phase_id: interval_seconds}
) -> pd.Series:
    """
    Erstellt Labels f√ºr zeitbasierte Vorhersagen
    
    Beispiel: "Steigt price_close in 10 Minuten um mindestens 5%?"
    
    ‚ö†Ô∏è WICHTIG: 
    - Daten m√ºssen nach timestamp sortiert sein!
    - Verwendet interval_seconds pro Phase aus ref_coin_phases (genauer als Durchschnitt!)
    
    Args:
        data: DataFrame mit Trainingsdaten (MUSS nach timestamp sortiert sein!)
        target_variable: Variable die √ºberwacht wird (z.B. "price_close")
        future_minutes: Anzahl Minuten in die Zukunft (z.B. 10)
        min_percent_change: Mindest-Prozent-√Ñnderung (z.B. 5.0 f√ºr 5%)
        direction: "up" (steigt) oder "down" (f√§llt)
    
    Returns:
        Series mit bin√§ren Labels (1 = Bedingung erf√ºllt, 0 = nicht erf√ºllt)
    """
    if target_variable not in data.columns:
        raise ValueError(f"Target-Variable '{target_variable}' nicht in Daten gefunden!")
    
    # ‚ö†Ô∏è WICHTIG: Daten m√ºssen nach timestamp sortiert sein!
    if not data.index.is_monotonic_increasing:
        data = data.sort_index()
        logger.warning("‚ö†Ô∏è Daten wurden nach timestamp sortiert")
    
    # Aktueller Wert
    current_values = data[target_variable]
    
    if len(data) < 2:
        raise ValueError("Nicht genug Daten f√ºr zeitbasierte Labels (mindestens 2 Zeilen ben√∂tigt)")
    
    # NEU: Verwende interval_seconds pro Phase aus ref_coin_phases (falls √ºbergeben)
    # Pr√ºfe ob phase_id_at_time vorhanden ist
    if 'phase_id_at_time' in data.columns and phase_intervals:
        # Verwende interval_seconds pro Phase (genauer!)
        logger.info(f"‚úÖ Verwende interval_seconds pro Phase aus ref_coin_phases")
        
        # Erstelle Series mit rows_to_shift pro Zeile basierend auf Phase
        def calculate_rows_to_shift(phase_id):
            if pd.isna(phase_id) or phase_id not in phase_intervals:
                # Fallback: Verwende Durchschnitt wenn Phase unbekannt
                time_diffs = data.index.to_series().diff().dropna()
                avg_interval_minutes = time_diffs.mean().total_seconds() / 60.0
                return int(round(future_minutes / avg_interval_minutes)) if avg_interval_minutes > 0 else 0
            
            interval_seconds = phase_intervals[phase_id]
            if interval_seconds <= 0:
                # Fallback f√ºr Phase 99 (Finished) mit interval_seconds = 0
                time_diffs = data.index.to_series().diff().dropna()
                avg_interval_minutes = time_diffs.mean().total_seconds() / 60.0
                return int(round(future_minutes / avg_interval_minutes)) if avg_interval_minutes > 0 else 0
            
            interval_minutes = interval_seconds / 60.0
            # ‚ö†Ô∏è WICHTIG: Verwende ceil() statt round() f√ºr konservativere Berechnung
            return max(1, int(np.ceil(future_minutes / interval_minutes)))
        
        # Berechne rows_to_shift pro Zeile
        rows_to_shift_series = data['phase_id_at_time'].apply(calculate_rows_to_shift)
        
        # Berechne zuk√ºnftige Werte pro Zeile (verschiedene Shifts je nach Phase)
        future_values = pd.Series(index=data.index, dtype=float)
        for idx in data.index:
            rows_to_shift_val = rows_to_shift_series.loc[idx]
            # Konvertiere zu int falls Series
            if isinstance(rows_to_shift_val, pd.Series):
                rows_to_shift_val = rows_to_shift_val.iloc[0] if len(rows_to_shift_val) > 0 else 0
            rows_to_shift = int(rows_to_shift_val) if not pd.isna(rows_to_shift_val) else 0
            
            if rows_to_shift > 0:
                # Finde Index nach rows_to_shift Zeilen
                try:
                    current_pos = data.index.get_loc(idx)
                    if isinstance(current_pos, slice):
                        current_pos = current_pos.start if current_pos.start is not None else 0
                    future_pos = current_pos + rows_to_shift
                    if future_pos < len(data.index):
                        future_idx = data.index[future_pos]
                        future_values.loc[idx] = data.loc[future_idx, target_variable]
                    else:
                        future_values.loc[idx] = np.nan
                except (IndexError, KeyError, TypeError):
                    # Am Ende des Datensatzes: kein Zukunftswert verf√ºgbar
                    future_values.loc[idx] = np.nan
            else:
                future_values.loc[idx] = np.nan
        
        logger.info(f"üìä Zeitbasierte Labels: {future_minutes} Minuten")
        logger.info(f"   Phase-Intervalle verwendet: {len(phase_intervals)} Phasen geladen")
        
    else:
        # Fallback: Verwende Durchschnitt (wenn phase_id_at_time nicht vorhanden)
        logger.warning("‚ö†Ô∏è phase_id_at_time nicht gefunden, verwende Durchschnitts-Intervall")
        
        time_diffs = data.index.to_series().diff().dropna()
        avg_interval_minutes = time_diffs.mean().total_seconds() / 60.0
        
        if avg_interval_minutes <= 0:
            raise ValueError(f"Ung√ºltiges Zeitintervall: {avg_interval_minutes} Minuten")
        
        # ‚ö†Ô∏è WICHTIG: Verwende ceil() statt round() f√ºr konservativere Berechnung
        # ceil() stellt sicher, dass wir mindestens genug Zeilen nehmen
        rows_to_shift = max(1, int(np.ceil(future_minutes / avg_interval_minutes)))
        
        if rows_to_shift <= 0:
            raise ValueError(f"future_minutes ({future_minutes}) ist kleiner als durchschnittliches Intervall ({avg_interval_minutes:.2f} Minuten)")
        
        logger.info(f"üìä Zeitbasierte Labels: {future_minutes} Minuten = ~{rows_to_shift} Zeilen (Intervall: {avg_interval_minutes:.2f} Min)")
        
        # Zuk√ºnftiger Wert (shift nach hinten, da wir in die Zukunft schauen)
        future_values = data[target_variable].shift(-rows_to_shift)
    
    # Berechne prozentuale √Ñnderung
    # ‚ö†Ô∏è WICHTIG: Konvertiere Decimal zu float (Datenbank liefert Decimal, Pandas braucht float)
    current_values = pd.to_numeric(current_values, errors='coerce')
    future_values = pd.to_numeric(future_values, errors='coerce')
    
    # ‚ö†Ô∏è WICHTIG: Vermeide Division durch Null
    # Erstelle Mask f√ºr g√ºltige Werte (current_values != 0 und nicht NaN)
    valid_mask = (current_values != 0) & current_values.notna() & future_values.notna()
    
    # Berechne Prozent-√Ñnderung nur f√ºr g√ºltige Werte
    percent_change = pd.Series(index=data.index, dtype=float)
    percent_change[valid_mask] = ((future_values[valid_mask] - current_values[valid_mask]) / current_values[valid_mask]) * 100
    
    # Setze ung√ºltige Werte auf NaN (werden sp√§ter behandelt)
    percent_change[~valid_mask] = np.nan
    
    # Erstelle Labels basierend auf Richtung
    if direction == "up":
        # Steigt um mindestens min_percent_change?
        labels = (percent_change >= min_percent_change).astype(float)  # float f√ºr NaN-Handling
    else:  # "down"
        # F√§llt um mindestens min_percent_change?
        labels = (percent_change <= -min_percent_change).astype(float)  # float f√ºr NaN-Handling
    
    # ‚ö†Ô∏è WICHTIG: Behandle NaN-Werte (am Ende des Datensatzes oder bei Null-Werten)
    nan_count = labels.isna().sum()
    if nan_count > 0:
        logger.warning(f"‚ö†Ô∏è {nan_count} Zeilen ohne g√ºltige Zukunftswerte (werden ausgeschlossen)")
        # Entferne Zeilen mit NaN aus Labels (und sp√§ter auch aus Daten)
        labels = labels.dropna()
        # Entferne entsprechende Zeilen aus Daten (wichtig f√ºr Alignment!)
        data = data.loc[labels.index]
        logger.info(f"üìä Nach NaN-Entfernung: {len(labels)} g√ºltige Labels")
    
    # Konvertiere zu int (nach NaN-Entfernung)
    labels = labels.astype(int)
    
    positive = labels.sum()
    negative = len(labels) - positive
    
    logger.info(f"‚úÖ Zeitbasierte Labels erstellt: {positive} positive, {negative} negative")
    logger.info(f"   Zeitraum: {future_minutes} Minuten, Min-√Ñnderung: {min_percent_change}%, Richtung: {direction}")
    
    # ‚ö†Ô∏è WICHTIG: Wenn Daten gefiltert wurden (NaN entfernt), gib auch gefilterte Daten zur√ºck
    # Pr√ºfe ob data und labels noch aligned sind
    if len(data) != len(labels):
        logger.warning(f"‚ö†Ô∏è Daten und Labels nicht aligned: {len(data)} Daten, {len(labels)} Labels")
        # Filtere Daten auf Labels-Index
        data = data.loc[labels.index]
        logger.info(f"üìä Daten gefiltert: {len(data)} Zeilen")
        return labels, data  # Gib beide zur√ºck
    
    return labels  # Normale R√ºckgabe (nur Labels)

def test_ath_features_isolated() -> bool:
    """
    SCHRITT 3: Teste ATH-Features isoliert

    Erstellt Test-Daten und pr√ºft, ob ATH-Features korrekt generiert werden.
    Gibt True zur√ºck wenn erfolgreich, False bei Fehlern.

    Returns:
        bool: True wenn ATH-Features erfolgreich generiert wurden
    """
    logger.info("üß™ SCHRITT 3: Teste ATH-Features isoliert")

    try:
        # Erstelle synthetische Test-Daten
        import pandas as pd
        from datetime import datetime, timezone

        data = {
            'price_high': [100, 105, 110, 115, 120, 125, 130, 135],
            'price_close': [98, 103, 108, 113, 118, 123, 128, 133],
            'mint': ['TEST123'] * 8,
            'price_open': [97, 102, 107, 112, 117, 122, 127, 132],
            'volume_sol': [1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700]
        }

        timestamps = [
            datetime(2025, 12, 31, 10, i, 0, tzinfo=timezone.utc)
            for i in range(8)
        ]

        df = pd.DataFrame(data, index=timestamps)
        logger.info(f"‚úÖ Test-DataFrame erstellt: {len(df)} Zeilen, Spalten: {list(df.columns)}")

        # Teste add_ath_features
        result_df = add_ath_features(df)

        # Pr√ºfe erwartete ATH-Features
        expected_ath = ['rolling_ath', 'ath_distance_pct', 'ath_breakout', 'minutes_since_ath', 'ath_age_hours', 'ath_is_recent', 'ath_is_old']
        created_ath = [col for col in result_df.columns if col in expected_ath]

        logger.info(f"üéØ Erwartete ATH-Features: {len(expected_ath)}")
        logger.info(f"‚úÖ Erstellte ATH-Features: {len(created_ath)} - {created_ath}")

        missing_ath = [f for f in expected_ath if f not in result_df.columns]
        if missing_ath:
            logger.error(f"‚ùå Fehlende ATH-Features: {missing_ath}")
            return False

        # Zus√§tzliche Validierung
        if 'rolling_ath' in result_df.columns:
            rolling_ath_values = result_df['rolling_ath'].head(3).tolist()
            logger.info(f"üìä rolling_ath Beispiel-Werte: {rolling_ath_values}")

        logger.info("üéâ ATH-Features erfolgreich isoliert getestet!")
        return True

    except Exception as e:
        logger.error(f"‚ùå ATH-Feature-Test fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()
        return False

def add_ath_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    HISTORISCH KORREKTE ATH-FEATURES: Berechnet ATH-Features ohne Data Leakage

    WICHTIG: Diese Funktion berechnet historische ATH-Werte basierend auf den Daten
    im DataFrame, nicht aus statischen Datenbank-Snapshots. Dadurch wird Data Leakage
    verhindert, da die KI nur historische Informationen nutzt.

    Features:
    - rolling_ath: Historisches All-Time-High bis zu jedem Zeitpunkt
    - ath_distance_pct: Wie weit ist der aktuelle Preis vom historischen ATH entfernt?
    - ath_breakout: Ist das aktuelle High h√∂her als das vorherige historische ATH?
    - minutes_since_ath: Wie viele Minuten sind seit dem letzten ATH vergangen?

    Args:
        df: DataFrame mit price_high, price_close, timestamp, mint Spalten

    Returns:
        DataFrame mit zus√§tzlichen ATH-Features
    """
    logger.info("üöÄ Berechne historische ATH-Features (Data Leakage-frei)")

    # Kopie erstellen
    df = df.copy()

    # üõ°Ô∏è ROBUSTHEIT: Pr√ºfe ob alle ben√∂tigten Spalten vorhanden sind
    required_columns = ['price_high', 'price_close', 'mint']
    missing_columns = [col for col in required_columns if col not in df.columns]

    if missing_columns:
        logger.warning(f"‚ö†Ô∏è ATH-Features k√∂nnen nicht berechnet werden - fehlende Spalten: {missing_columns}")
        logger.info("üìä Verwende Original-Daten ohne ATH-Features")
        return df

    # Stelle sicher, dass timestamp als Spalte verf√ºgbar ist
    if 'timestamp' not in df.columns:
        if isinstance(df.index, pd.DatetimeIndex):
            # Timestamp ist DatetimeIndex - f√ºge als Spalte hinzu
            logger.info(f"üîÑ Konvertiere DatetimeIndex zu timestamp-Spalte f√ºr ATH-Berechnung")
            df = df.reset_index()  # Konvertiert Index zu Spalte

            # Die resultierende Spalte k√∂nnte 'index' oder df.index.name hei√üen
            if 'index' in df.columns and 'timestamp' not in df.columns:
                df = df.rename(columns={'index': 'timestamp'})
            elif df.index.name and df.index.name in df.columns and df.index.name != 'timestamp':
                df = df.rename(columns={df.index.name: 'timestamp'})

            logger.info(f"‚úÖ DatetimeIndex zu timestamp-Spalte konvertiert. Spalten: {list(df.columns)}")
        else:
            logger.warning(f"‚ö†Ô∏è ATH-Features k√∂nnen nicht berechnet werden - kein DatetimeIndex verf√ºgbar")
            logger.info("üìä Verwende Original-Daten ohne ATH-Features")
            return df
    else:
        logger.info("‚úÖ Timestamp-Spalte bereits vorhanden")

    # KRITISCH: Entferne jeglichen timestamp-Index um Konflikte zu vermeiden
    if isinstance(df.index, pd.DatetimeIndex) or df.index.name == 'timestamp':
        logger.info("üîÑ Entferne timestamp-Index zur Vermeidung von Index/Spalten-Konflikten")
        df = df.reset_index(drop=True)

    # Sortiere nach mint und timestamp (wichtig f√ºr historische Berechnung!)
    df = df.sort_values(['mint', 'timestamp'])

    # 1. ROLLING ATH berechnen (historisches All-Time-High)
    df['rolling_ath'] = df.groupby('mint')['price_high'].cummax()

    # 2. ATH_DISTANCE_PCT: Wie weit entfernt vom ATH?
    # Formel: (ATH - Close) / ATH * 100
    # Negativ = unter ATH, positiv = √ºber ATH
    df['ath_distance_pct'] = ((df['rolling_ath'] - df['price_close']) / df['rolling_ath'].replace(0, np.nan)) * 100

    # 3. ATH_BREAKOUT: Neue ATH-Breaks
    # Ist das aktuelle High h√∂her als das vorherige historische ATH?
    df['prev_rolling_ath'] = df.groupby('mint')['rolling_ath'].shift(1)
    df['ath_breakout'] = (df['price_high'] > df['prev_rolling_ath']).astype(int)

    # 4. MINUTES_SINCE_ATH: Zeit seit letztem ATH
    # Finde den Timestamp des letzten ATH-Breakouts
    def calculate_minutes_since_ath(group):
        # Finde alle Zeiten wo ein neuer ATH erreicht wurde
        ath_timestamps = group.loc[group['ath_breakout'] == 1, 'timestamp']

        if len(ath_timestamps) == 0:
            # Kein ATH-Breakout gefunden, verwende ersten Timestamp als Baseline
            group['minutes_since_ath'] = (group['timestamp'] - group['timestamp'].iloc[0]).dt.total_seconds() / 60
        else:
            # Berechne Zeit seit letztem ATH-Breakout
            group['last_ath_timestamp'] = ath_timestamps.reindex(group.index, method='ffill')
            group['minutes_since_ath'] = (group['timestamp'] - group['last_ath_timestamp']).dt.total_seconds() / 60

        return group

    df = df.groupby('mint', group_keys=False).apply(calculate_minutes_since_ath)

    # 5. Zus√§tzliche n√ºtzliche ATH-Features
    df['ath_age_hours'] = df['minutes_since_ath'] / 60.0
    df['ath_is_recent'] = (df['minutes_since_ath'] < 60).astype(int)  # Innerhalb 1 Stunde
    df['ath_is_old'] = (df['minutes_since_ath'] > 1440).astype(int)  # √Ñlter als 24 Stunden

    # NaN-Werte sinnvoll behandeln
    df['minutes_since_ath'].fillna(0, inplace=True)  # Am Anfang: 0 Minuten
    df['ath_age_hours'].fillna(0, inplace=True)
    df['ath_distance_pct'].fillna(0, inplace=True)  # Wenn kein ATH: 0%

    # Hilfsspalten entfernen
    df.drop(['prev_rolling_ath', 'last_ath_timestamp'], axis=1, inplace=True, errors='ignore')

    # Stelle sicher, dass timestamp als Index gesetzt ist (f√ºr Konsistenz)
    if 'timestamp' in df.columns:
        if isinstance(df.index, pd.DatetimeIndex):
            # Timestamp ist bereits Index, entferne die Spalte
            df.drop('timestamp', axis=1, inplace=True)
            logger.info("‚úÖ Timestamp-Spalte entfernt (bereits als Index vorhanden)")
        else:
            # Timestamp ist Spalte, setze als Index
            df.set_index('timestamp', inplace=True)
            logger.info("‚úÖ Timestamp als Index gesetzt")

    logger.info(f"‚úÖ Historische ATH-Features erstellt: {len(df)} Zeilen")
    logger.info(f"   Features: rolling_ath, ath_distance_pct, ath_breakout, minutes_since_ath")

    return df

def create_pump_detection_features(
    data: pd.DataFrame,
    window_sizes: list = [5, 10, 15]
) -> pd.DataFrame:
    """
    MODERNISIERT: Nutzt neue Metriken aus coin_metrics.
    Erstellt zus√§tzliche Features f√ºr Pump-Detection.
    
    Args:
        data: DataFrame mit coin_metrics Daten (MUSS nach timestamp sortiert sein!)
        window_sizes: Fenstergr√∂√üen f√ºr Rolling-Berechnungen (in Anzahl Zeilen)
    
    Returns:
        DataFrame mit zus√§tzlichen Features (urspr√ºngliche Features bleiben erhalten)
    """
    df = data.copy()

    # ‚ö†Ô∏è WICHTIG: Daten m√ºssen nach timestamp sortiert sein!
    if not df.index.is_monotonic_increasing:
        df = df.sort_index()
        logger.warning("‚ö†Ô∏è Daten wurden nach timestamp sortiert f√ºr Feature-Engineering")

    # üß† Pr√ºfe vorhandene Features
    existing_features = set(df.columns)
    created_features = []  # Sammle erstellte Features
    logger.info(f"üìä DataFrame hat bereits {len(existing_features)} Features aus Datenbank")

    # ‚úÖ Pump-System speichert bereits engineered Features!
    # create_pump_detection_features wird nur noch f√ºr etwaige fehlende Features verwendet
    logger.info("üß† Pr√ºfe auf fehlende engineered Features...")

    try:
        # Stelle sicher, dass timestamp nicht als Spalte existiert (nur als Index)
        if 'timestamp' in df.columns:
            logger.warning("‚ö†Ô∏è Timestamp ist als Spalte vorhanden - entferne sie zur Vermeidung von Konflikten")
            df.drop('timestamp', axis=1, inplace=True)

        df = add_ath_features(df)
        logger.info(f"‚úÖ ATH-Features erfolgreich hinzugef√ºgt: rolling_ath, ath_distance_pct, ath_breakout, minutes_since_ath")

        # Validiere dass ATH-Features erstellt wurden
        ath_features_created = [col for col in df.columns if col in ['rolling_ath', 'ath_distance_pct', 'ath_breakout', 'minutes_since_ath']]
        logger.info(f"üéØ ATH-Features validiert: {len(ath_features_created)} von 4 erstellt")

    except Exception as e:
        logger.error(f"‚ùå ATH-Features konnten nicht reaktiviert werden: {e}")
        logger.warning("‚ö†Ô∏è Falle zur√ºck auf Daten ohne ATH-Features")
        # Bei Fehlern: Fortfahren ohne ATH-Features (sicherer Fallback)

    # ‚úÖ 1. Dev-Tracking Features (KRITISCH!)
    # ROBUST: Erstelle Features auch wenn Spalte fehlt oder NULL ist
    if 'dev_sold_amount' in df.columns:
        dev_sold_clean = df['dev_sold_amount'].fillna(0)
        df['dev_sold_flag'] = (dev_sold_clean > 0).astype(int)
        df['dev_sold_cumsum'] = dev_sold_clean.cumsum()
        for window in window_sizes:
            df[f'dev_sold_spike_{window}'] = (
                dev_sold_clean.rolling(window, min_periods=1).sum() > 0
            ).astype(int)
    else:
        # Fallback: Erstelle Features mit 0-Werten wenn Spalte fehlt
        logger.warning("‚ö†Ô∏è dev_sold_amount fehlt - erstelle Dev-Features mit 0-Werten")
        df['dev_sold_flag'] = 0
        df['dev_sold_cumsum'] = 0.0
        for window in window_sizes:
            df[f'dev_sold_spike_{window}'] = 0
    
    # ‚úÖ 2. Ratio-Features (schon berechnet in coin_metrics!)
    # ROBUST: Erstelle Features auch wenn Spalte fehlt
    if 'buy_pressure_ratio' in df.columns:
        buy_pressure_clean = df['buy_pressure_ratio'].fillna(0)
        for window in window_sizes:
            df[f'buy_pressure_ma_{window}'] = (
                buy_pressure_clean.rolling(window, min_periods=1).mean()
            )
            df[f'buy_pressure_trend_{window}'] = (
                buy_pressure_clean - df[f'buy_pressure_ma_{window}']
            )
    else:
        logger.warning("‚ö†Ô∏è buy_pressure_ratio fehlt - erstelle Ratio-Features mit 0-Werten")
        for window in window_sizes:
            df[f'buy_pressure_ma_{window}'] = 0.0
            df[f'buy_pressure_trend_{window}'] = 0.0
    
    # ‚úÖ 3. Whale-Aktivit√§t Features
    # ROBUST: Erstelle Features auch wenn Spalten fehlen
    if 'whale_buy_volume_sol' in df.columns and 'whale_sell_volume_sol' in df.columns:
        whale_buy_clean = df['whale_buy_volume_sol'].fillna(0)
        whale_sell_clean = df['whale_sell_volume_sol'].fillna(0)
        df['whale_net_volume'] = whale_buy_clean - whale_sell_clean
        for window in window_sizes:
            df[f'whale_activity_{window}'] = (
                whale_buy_clean.rolling(window, min_periods=1).sum() +
                whale_sell_clean.rolling(window, min_periods=1).sum()
            )
    else:
        logger.warning("‚ö†Ô∏è whale_buy_volume_sol oder whale_sell_volume_sol fehlen - erstelle Whale-Features mit 0-Werten")
        df['whale_net_volume'] = 0.0
        for window in window_sizes:
            df[f'whale_activity_{window}'] = 0.0
    
    # ‚úÖ 4. Volatilit√§ts-Features (nutzt neue volatility_pct Spalte!)
    if 'volatility_pct' in df.columns:
        for window in window_sizes:
            df[f'volatility_ma_{window}'] = (
                df['volatility_pct'].rolling(window, min_periods=1).mean()
            )
            df[f'volatility_spike_{window}'] = (
                df['volatility_pct'] > 
                df[f'volatility_ma_{window}'] * 1.5
            ).astype(int)
    
    # ‚úÖ 5. Wash-Trading Detection
    if 'unique_signer_ratio' in df.columns:
        for window in window_sizes:
            df[f'wash_trading_flag_{window}'] = (
                df['unique_signer_ratio'].rolling(window, min_periods=1).mean() < 0.15
            ).astype(int)
    
    # ‚úÖ 6. Net-Volume Features
    if 'net_volume_sol' in df.columns:
        for window in window_sizes:
            df[f'net_volume_ma_{window}'] = (
                df['net_volume_sol'].rolling(window, min_periods=1).mean()
            )
            df[f'volume_flip_{window}'] = (
                (df['net_volume_sol'] > 0).astype(int).diff().abs()
            )
    
    # ‚úÖ 7. Price Momentum (nutzt price_close)
    if 'price_close' in df.columns:
        for window in window_sizes:
            df[f'price_change_{window}'] = df['price_close'].pct_change(periods=window) * 100
            df[f'price_roc_{window}'] = (
                (df['price_close'] - df['price_close'].shift(window)) / 
                df['price_close'].shift(window).replace(0, np.nan)
            ) * 100
    
    # ‚úÖ 8. Volume Patterns (nutzt volume_sol)
    if 'volume_sol' in df.columns:
        for window in window_sizes:
            rolling_avg = df['volume_sol'].rolling(window=window, min_periods=1).mean()
            df[f'volume_ratio_{window}'] = df['volume_sol'] / rolling_avg.replace(0, np.nan)
            rolling_std = df['volume_sol'].rolling(window=window, min_periods=1).std()
            df[f'volume_spike_{window}'] = (
                (df['volume_sol'] - rolling_avg) / rolling_std.replace(0, np.nan)
            )
    
    # ‚úÖ 9. Market Cap Velocity
    if 'market_cap_close' in df.columns:
        for window in window_sizes:
            df[f'mcap_velocity_{window}'] = (
                (df['market_cap_close'] - df['market_cap_close'].shift(window)) / 
                df['market_cap_close'].shift(window).replace(0, np.nan)
            ) * 100
    
    # üÜï 10. ATH-basierte Rolling-Window Features (auf bereits berechneten historischen ATH-Features)
    # Die historischen ATH-Features werden bereits in add_ath_features() berechnet
    if 'ath_distance_pct' in df.columns and 'ath_breakout' in df.columns:
        # Rolling-Windows f√ºr ATH-Features (auf historisch korrekten Werten)
        for window in window_sizes:
            # ATH-Trend (n√§hert sich Preis dem ATH?)
            df[f'ath_distance_trend_{window}'] = (
                df['ath_distance_pct'].rolling(window, min_periods=1).mean()
            )
            df[f'ath_approach_{window}'] = (
                df[f'ath_distance_trend_{window}'].diff() < 0
            ).astype(int)  # N√§hert sich dem ATH

            # ATH-Breakout-H√§ufigkeit
            df[f'ath_breakout_count_{window}'] = (
                df['ath_breakout'].rolling(window, min_periods=1).sum()
            )

            # ATH-Volumen bei Breakouts
            if 'volume_sol' in df.columns:
                ath_breakout_volume = df['ath_breakout'] * df['volume_sol']
                df[f'ath_breakout_volume_ma_{window}'] = (
                    ath_breakout_volume.rolling(window, min_periods=1).mean()
                )

    # üÜï 11. ATH-Zeit-Features Rolling-Windows (auf bereits berechneten historischen Werten)
    if 'minutes_since_ath' in df.columns:
        for window in window_sizes:
            feature_name = f'ath_age_trend_{window}'
            if feature_name not in existing_features:
                df[feature_name] = (
                    df['minutes_since_ath'].rolling(window, min_periods=1).mean()
                )
                logger.debug(f"‚ûï Erstellt: {feature_name}")
            else:
                logger.debug(f"‚è≠Ô∏è √úberspringe (bereits vorhanden): {feature_name}")
    
    # NaN-Werte durch 0 ersetzen (entstehen durch Rolling/Shift)
    df.fillna(0, inplace=True)
    
    # Infinite Werte durch 0 ersetzen
    df.replace([np.inf, -np.inf], 0, inplace=True)
    
    engineered_count = len([c for c in df.columns if c not in data.columns])
    logger.info(f"‚úÖ {engineered_count} zus√§tzliche Features erstellt")
    
    return df


def get_engineered_feature_names(window_sizes: list = [5, 10, 15]) -> list:
    """
    Gibt die Namen aller erstellten Features zur√ºck.
    N√ºtzlich f√ºr Feature-Auswahl in UI und Feature Importance.
    
    ‚ö†Ô∏è WICHTIG: Diese Liste muss mit create_pump_detection_features() √ºbereinstimmen!
    
    Args:
        window_sizes: Fenstergr√∂√üen (muss mit create_pump_detection_features() √ºbereinstimmen)
    
    Returns:
        Liste der Feature-Namen
    """
    features = []
    
    # ‚úÖ 1. Dev-Tracking Features
    features.extend(['dev_sold_flag', 'dev_sold_cumsum'])
    for w in window_sizes:
        features.append(f'dev_sold_spike_{w}')
    
    # ‚úÖ 2. Ratio-Features
    for w in window_sizes:
        features.extend([f'buy_pressure_ma_{w}', f'buy_pressure_trend_{w}'])
    
    # ‚úÖ 3. Whale-Aktivit√§t Features
    features.append('whale_net_volume')
    for w in window_sizes:
        features.append(f'whale_activity_{w}')
    
    # ‚úÖ 4. Volatilit√§ts-Features
    for w in window_sizes:
        features.extend([f'volatility_ma_{w}', f'volatility_spike_{w}'])
    
    # ‚úÖ 5. Wash-Trading Detection
    for w in window_sizes:
        features.append(f'wash_trading_flag_{w}')
    
    # ‚úÖ 6. Net-Volume Features
    for w in window_sizes:
        features.extend([f'net_volume_ma_{w}', f'volume_flip_{w}'])
    
    # ‚úÖ 7. Price Momentum
    for w in window_sizes:
        features.extend([f'price_change_{w}', f'price_roc_{w}'])
    
    # ‚úÖ 8. Volume Patterns
    for w in window_sizes:
        features.extend([f'volume_ratio_{w}', f'volume_spike_{w}'])
    
    # ‚úÖ 9. Market Cap Velocity
    for w in window_sizes:
        features.append(f'mcap_velocity_{w}')
    
    # üÜï 10. ATH-basierte Rolling-Window Features
    # Hinweis: ath_distance_pct, ath_breakout, ath_age_hours, ath_is_recent, ath_is_old
    # werden bereits in add_ath_features() erstellt und sind nicht in dieser Liste

    # ATH Rolling-Window Features
    for w in window_sizes:
        features.extend([
            f'ath_distance_trend_{w}',
            f'ath_approach_{w}',
            f'ath_breakout_count_{w}',
            f'ath_breakout_volume_ma_{w}',
            f'ath_age_trend_{w}'
        ])
    
    return features

# Kritische Features f√ºr Rug-Detection
CRITICAL_FEATURES = [
    "dev_sold_amount",  # KRITISCH: Rug-Pull-Indikator
    "buy_pressure_ratio",  # Relatives Buy/Sell-Verh√§ltnis
    "unique_signer_ratio",  # Wash-Trading-Erkennung
    "whale_buy_volume_sol",
    "whale_sell_volume_sol",
    "net_volume_sol",
    "volatility_pct",
    "ath_distance_pct",  # üÜï KRITISCH: Historische ATH-Distance (Data Leakage-frei)
    "ath_breakout",  # üÜï KRITISCH: Historische ATH-Breakouts
    "minutes_since_ath"  # üÜï KRITISCH: Zeit seit letztem ATH
]

def get_available_ath_features(include_ath: bool = True) -> list:
    """
    Gibt die verf√ºgbaren ATH-Features zur√ºck (wenn ATH aktiviert ist).

    Args:
        include_ath: Ob ATH-Features berechnet wurden

    Returns:
        Liste der verf√ºgbaren ATH-Features
    """
    if not include_ath:
        return []

    return [
        'rolling_ath',
        'ath_distance_pct',
        'ath_breakout',
        'minutes_since_ath',
        'ath_age_hours',
        'ath_is_recent',
        'ath_is_old'
    ]

def validate_critical_features(features: List[str]) -> Dict[str, bool]:
    """
    Pr√ºft ob kritische Features verwendet werden.
    
    Args:
        features: Liste der Feature-Namen
    
    Returns:
        Dict mit {feature_name: bool} - True wenn Feature vorhanden
    """
    return {
        feature: feature in features 
        for feature in CRITICAL_FEATURES
    }


async def validate_ath_data_availability(
    train_start: datetime | str,
    train_end: datetime | str
) -> Dict[str, Any]:
    """
    Pr√ºft ob ATH-Daten f√ºr den Zeitraum verf√ºgbar sind.
    
    Args:
        train_start: Start-Zeitpunkt
        train_end: Ende-Zeitpunkt
    
    Returns:
        Dict mit:
        - available: bool
        - coins_with_ath: int
        - coins_without_ath: int
        - coverage_pct: float
        - total_coins: int
    """
    pool = await get_pool()
    
    # Konvertiere zu UTC
    train_start_utc = _ensure_utc(train_start)
    train_end_utc = _ensure_utc(train_end)
    
    # Pr√ºfe wie viele Coins ATH-Daten haben
    query = """
        SELECT 
            COUNT(DISTINCT cm.mint) as total_coins,
            COUNT(DISTINCT CASE WHEN COALESCE(cs.ath_price_sol, 0) > 0 THEN cm.mint END) as coins_with_ath,
            COUNT(DISTINCT CASE WHEN COALESCE(cs.ath_price_sol, 0) = 0 OR cs.ath_price_sol IS NULL THEN cm.mint END) as coins_without_ath
        FROM coin_metrics cm
        LEFT JOIN coin_streams cs ON cm.mint = cs.token_address
        WHERE cm.timestamp >= $1 AND cm.timestamp <= $2
    """
    
    try:
        row = await pool.fetchrow(query, train_start_utc, train_end_utc)
        
        total_coins = row['total_coins'] or 0
        coins_with_ath = row['coins_with_ath'] or 0
        coins_without_ath = row['coins_without_ath'] or 0
        
        coverage_pct = (coins_with_ath / total_coins * 100) if total_coins > 0 else 0.0
        
        return {
            "available": coins_with_ath > 0,
            "coins_with_ath": coins_with_ath,
            "coins_without_ath": coins_without_ath,
            "coverage_pct": coverage_pct,
            "total_coins": total_coins
        }
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Fehler bei ATH-Daten-Validierung: {e}")
        return {
            "available": False,
            "coins_with_ath": 0,
            "coins_without_ath": 0,
            "coverage_pct": 0.0,
            "total_coins": 0,
            "error": str(e)
        }


def check_overlap(
    train_start: datetime | str,
    train_end: datetime | str,
    test_start: datetime | str,
    test_end: datetime | str
) -> Dict[str, Any]:
    """
    Pr√ºft ob Test-Zeitraum sich mit Training √ºberschneidet
    
    ‚ö†Ô∏è Wichtig: Gibt Warnung zur√ºck, blockiert aber NICHT den Test!
    
    Args:
        train_start: Training Start-Zeitpunkt
        train_end: Training Ende-Zeitpunkt
        test_start: Test Start-Zeitpunkt
        test_end: Test Ende-Zeitpunkt
    
    Returns:
        Dict mit has_overlap (bool) und overlap_note (str)
    """
    # Konvertiere zu UTC
    train_start_utc = _ensure_utc(train_start)
    train_end_utc = _ensure_utc(train_end)
    test_start_utc = _ensure_utc(test_start)
    test_end_utc = _ensure_utc(test_end)
    
    # Pr√ºfe √úberschneidung
    train_duration = (train_end_utc - train_start_utc).total_seconds()
    test_duration = (test_end_utc - test_start_utc).total_seconds()
    
    # Berechne √úberschneidung
    overlap_start = max(train_start_utc, test_start_utc)
    overlap_end = min(train_end_utc, test_end_utc)
    
    if overlap_start < overlap_end:
        overlap_duration = (overlap_end - overlap_start).total_seconds()
        overlap_percent = (overlap_duration / test_duration) * 100 if test_duration > 0 else 0
        
        return {
            "has_overlap": True,
            "overlap_note": f"‚ö†Ô∏è {overlap_percent:.1f}% √úberschneidung mit Trainingsdaten - Ergebnisse k√∂nnen verf√§lscht sein"
        }
    else:
        return {
            "has_overlap": False,
            "overlap_note": "‚úÖ Keine √úberschneidung mit Trainingsdaten"
        }

